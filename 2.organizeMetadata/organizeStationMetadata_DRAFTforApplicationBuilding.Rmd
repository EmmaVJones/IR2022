---
title: "Organize Station Metadata"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(tidyverse)
library(readxl)
library(sf)
```

## Overview

This project picks up after regional assessment staff attribute WQS and AU information to each station identified for inclusion in a given assessment cycle. 

This script combines the WQS and AU information archived by those applications (from the R Connect server) and attaches that information to all data associated with an assessment window.

This script combines final data, as opposed to draft data. Previous iterations used draft data to assist application rebuilding processes prior to final data release. Relics of that information may be seen in the script comments.

### Data Organization

In order to link all data in the IR window to the appropriate station information, we need to bring in a few preliminary datasets.

* Conventionals (final)
* Last cycle AU information (still draft)
* Station Attributes (from 1.preprocessingData steps)
    + WQS information
    + AU information (specifically for stations that did not exist in last cycle)
* WQA CEDS Station table template (bulk data upload template)
    + New for 2022IR cycle, DEQ is moving from ADB to a more ATTAINS-like system comprised of a module in CEDS known as CEDS WQA. The development team has released a template for the 'bulk data upload' component where assessors may upload station information (manually or automatically assessed) to expedite the population of the CEDS module. We need to use this new data model as a starting point as it contains information for AU attribution, critical to WQA CEDS and automated assessment tools.
* PCB Data from Mark


The following sections will detail bringing in these necessary data sources and organizing them in a format the automated scripts can digest.

#### Conventionals (draft)

This dataset is the draft data sent out by Roger in Februrary 2021 prior to the official start of the assessment.

New method, matching Roland's citmon/nonagency database.

```{r}
source('updateConventionalsSchema.R')
```

##### Citmon/NonAgency Data

This is a tough one. The 2022IR is the first time DEQ is kinda getting things together and suggesting that non agency data comes to the assessement group with some sort of format, but that only works for 2019-2020 data. The BRRO assessors worked for weeks in 2020IR to reorganize their citmon/nonagency data and stations, so we are putting our best effort towards including that organized data into this assessment cycle. However, the way stations are organized with AU and WQS information will continue to be a problem until a more permanent solution with an official citmon database is developed.

```{r}
#source('addInBRRO2020IRcitmonData.R')
```



Old method.
```{r conventionals (draft), eval = FALSE}
#conventionals <- read_csv('data/final2020data/CEDSWQM_2020_IR_DATA-CONVENTIONALS_20190305.csv') %>%
#  # change to naming system 
#  rename("FDT_TEMP_CELCIUS"  ="TEMPERATURE_00010_DEGREES CENTIGRADE",
#         #"FDT_TEMP_CELCIUS_RMK" = "FDT_TEMP_CELCIUS_RMK",  
#         "FDT_FIELD_PH" = "pH_00400_STD_UNITS" ,          
#         #"FDT_FIELD_PH_RMK"  ="FDT_FIELD_PH_RMK", 
#         "DO" =  "DO_mg/L",       
#         #"DO_RMK"  ="DO_RMK",    
#         "FDT_SPECIFIC_CONDUCTANCE"="SPECIFIC_CONDUCTANCE_00095_UMHOS/CM @ 25C",    
#         #"FDT_SPECIFIC_CONDUCTANCE_RMK" ="FDT_SPECIFIC_CONDUCTANCE_RMK" ,
#         "FDT_SALINITY" = "SALINITY_00480_PPT" ,            
#         #"FDT_SALINITY_RMK"  ="FDT_SALINITY_RMK",  
#         "NITROGEN" = "NITROGEN_mg/L" ,                    
#         "AMMONIA" ="AMMONIA_mg/L",
#         "PHOSPHORUS" =  "PHOSPHORUS_mg/L",
#         "HARDNESS" = "HARDNESS_TOTAL_00900_mg/L",
#         "RMK_HARDNESS" = "RMK_00900",
#         "FECAL_COLI" = "FECAL_COLIFORM_31616_NO/100mL" ,
#         "RMK_FECAL_COLI" = "RMK_31616",
#         "E.COLI" = "ECOLI_CFU/100mL",                       
#         "ENTEROCOCCI" =  "ENTEROCOCCI_31649_NO/100mL",
#         "RMK_ENTEROCOCCI" =  "RMK_31649",
#         "CHLOROPHYLL" ="CHLOROPHYLL_32211_ug/L", 
#         "RMK_CHLOROPHYLL" = "RMK_32211",
#         "SSC" ="SSC-TOTAL_00530_mg/L" , 
#         "NITRATE" ="NITRATE_mg/L", 
#         "CHLORIDE" ="CHLORIDE_mg/L",
#         "SULFATE_TOTAL" = "SULFATE_TOTAL_00945_mg/L",  
#         "RMK_SULFATE_TOTAL" = "RMK_00945",
#         "SULFATE_DISS" ="SULFATE_DISSOLVED_00946_mg/L",
#         "RMK_SULFATE_DISS" = "RMK_00946") %>%
#  mutate(FDT_DATE_TIME = as.POSIXct(FDT_DATE_TIME, format="%m/%d/%Y %H:%M")) # fix date time
```

Filter draft conventionals (2013-2019) to current cycle (2015-2020).

```{r conventionals filter}
conventionals <- filter(conventionals, FDT_DATE_TIME > "2014-12-31")
```

And remove the offenders marked by assessors during metadata attribution process that need to be cut from the conventionals pull and not assessed. 

```{r station hit list}
hitList <- read_csv('data/stationHitList.csv')

conventionals <- filter(conventionals, ! FDT_STA_ID %in% toupper(hitList$`StationID to Remove`))
rm(hitList)
```


And now let's make a dataset of all the unique stations that we need to organize.

```{r conventionals_distinct}
conventionals_distinct <- conventionals %>%
  distinct(FDT_STA_ID, .keep_all = T) %>%
  # remove any data to avoid confusion
  dplyr::select(FDT_STA_ID:FDT_COMMENT, Latitude:Data_Source) %>%
  filter(!is.na(FDT_STA_ID))

# and make a spatial version
conventionals_sf <- conventionals_distinct %>%
  st_as_sf(coords = c("Longitude", "Latitude"), 
               remove = F, # don't remove these lat/lon cols from df
               crs = 4326) # add projection, needs to be geographic for now bc entering lat/lng

```


### Secchi Depth Query Detour

Secchi depth has not been pulled in the conventionals dataset but is required in the lakes assessment process. Where available, attach that information from ODS production.

```{r establish ODS connection}
library(pool)
### Production Environment
pool <- dbPool(
  drv = odbc::odbc(),
  Driver = "ODBC Driver 11 for SQL Server", #"SQL Server Native Client 11.0", 
  Server= "DEQ-SQLODS-PROD,50000",
  dbname = "ODS",
  trusted_connection = "yes"
)

#station <- '4AROA167.34'

stationSecchiDepth <- pool %>% tbl("Wqm_Field_Data_View") %>%
  filter(Fdt_Sta_Id %in% !! conventionals_distinct$FDT_STA_ID & #station & #
           between(as.Date(Fdt_Date_Time), "2014-12-31", "2020-12-31") &
           !is.na(Fdt_Secchi_Depth)) %>% # x >= left & x <= right
  dplyr::select(Fdt_Sta_Id, Fdt_Date_Time, Fdt_Depth, Fdt_Secchi_Depth) %>%
  as_tibble() %>%
  mutate(Date = as.Date(Fdt_Date_Time)) %>%
    dplyr::select(FDT_STA_ID = Fdt_Sta_Id, Date, FDT_DEPTH = Fdt_Depth, Fdt_Secchi_Depth) # name conventionals format
  #mutate(RMK_SECCHI_DEPTH = as.character(NA), LEVEL_SECCHI_DEPTH = as.factor(NA)) 
```

Join this data to conventionals data in the new conventionals format.

```{r secchi to conventionals}
conventionalsArchive <- conventionals %>%
  mutate(Date = as.Date(FDT_DATE_TIME))

#lake <- filter(conventionalsArchive, FDT_STA_ID == '4AROA167.34') %>%
#    mutate(Date = as.Date(FDT_DATE_TIME)) %>%
#  dplyr::select(FDT_STA_ID, FDT_DATE_TIME, FDT_DEPTH,  Date, SECCHI_DEPTH_M)

#lake2 <- left_join(lake, stationSecchiDepth, by = c('FDT_STA_ID', 'Date', 'FDT_DEPTH')) %>%
conventionals <- left_join(conventionalsArchive, stationSecchiDepth, by = c('FDT_STA_ID', 'Date', 'FDT_DEPTH')) %>%
  mutate(SECCHI_DEPTH_M = Fdt_Secchi_Depth) %>%
  dplyr::select(-c(Fdt_Secchi_Depth, Date))
  
#glimpse(conventionals)
#View(filter(conventionals, !is.na(SECCHI_DEPTH_M)))
```



And bring in Virginia assessment region info and DCR11 watershed info.

```{r vahu6}
vahu6 <- st_read('data/GIS/AssessmentRegions_VA84_basins.shp')
dcr11 <- st_read('data/GIS/dcr11_dd.shp') %>%
  st_transform(4326)
```


#### Stations Bulk Upload Template 

The biggest changes from the old stations table format to this new template is the addition of the 10 ID305B and station type columns as well as the lacustrine designation. 

```{r bulk upload template}
stationsTemplate <- read_excel('WQA_CEDS_templates/WQA_Bulk_Station_Upload_Final.xlsx',#'WQA_CEDS_templates/WQA_Bulk_Station_Upload (3).xlsx', 
                               sheet = 'Stations', col_types = "text")[0,] # just want structure and not the draft data, force everything to character for now
```


#### Last cycle AU information (draft)

Let's start by populating this template with draft 2020 stations table information. This is basically the best we can do for now given that 2020 spatial data is still not final as of late March 2021. This is just to get the assessors started, and Cleo has assured that most of this is final final information.

```{r last cycle stations}
lastCycleStation_sf <- st_read('data/GIS/2020_wqms.shp') %>%
  mutate(STATION_ID = toupper(STATION_ID)) # fix messed up stationID names to prevent joining issues
```

We first need to fix the comment field that comes with the above dataset since converting the original data to .shp truncates the information in the comment field. Below we bring in the original comment field and overwrite the shortened one with the real data.

```{r 2020 real comments}
realComments <- read_excel('data/GIS/ir20_Appendix8_StationsKey-List.xlsx', sheet = 'ir20_Appendix8_StationsList') %>%
  dplyr::select(STATION_ID, COMMENTS)

lastCycleStation_sf <- left_join(lastCycleStation_sf, realComments, by = "STATION_ID") %>%
  mutate(COMMENTS.x = COMMENTS.y) %>%
  rename('COMMENTS' = 'COMMENTS.x') %>%
  dplyr::select(-COMMENTS.y)
```



We want to drop stations that did not get sampled in 2022 cycle (**based on conventionals dataset for now**), but new for 2022IR we want to include stations that aren't fully supporting (S) for any reason, even if they don't have new data. This *may* be the catch needed to help assessors carry over stations cycle to cycle and see them in the apps.

```{r stations with data or carryover stations}
# Stations that should be kept even though they don't have data in window
lastCycleStation_keep <- filter_at(lastCycleStation_sf, vars(contains("_STAT")), any_vars(. %in% c("IM"))) %>% # find stations that had data in last window that need to carry over %>%
  rbind(filter(lastCycleStation_sf, str_detect(COMMENTS, 'carr'))) %>% # search for text about things 'carr'ying over, not perfect but will catch a lot
  distinct(STATION_ID, .keep_all = T) %>%
  st_drop_geometry()


# things could get difficult with bacteria carry over rules that are TBD, Stay tuned
  
# Stations that should be kept because they have data in the window
lastCycleStation_keepers <- filter(lastCycleStation_sf, STATION_ID %in% conventionals_distinct$FDT_STA_ID) %>%
  st_drop_geometry() %>%
  bind_rows(lastCycleStation_keep) %>% 
  distinct(STATION_ID, ID305B_1, ID305B_2, ID305B_3, .keep_all = T) # run distinct on STATION_ID and AUs because there are still stations assessed with more than on method and thus require 2 rows to accurately represent station in cycle (e.g. 9-PKC004.65, 4AROA004.54)

# find problem stations
View(
  lastCycleStation_keepers %>%
  group_by(STATION_ID) %>%
  mutate(n=n()) %>%
  filter(n >1))
rm(lastCycleStation_keep)
```

Now time to add in stations that did not get assessed in last cycle.

##### AU information (specifically for stations that did not exist in last cycle)

# Aside: organize AU information after it is pulled from the server

```{r organize AU metadata from server}
#source('organizeAUmetadataFromServer.R')
```



Bring in AU information assessors have chosen as a starting point for assessing new stations.

```{r newAU information}
# most recent version taken from ..\1.preprocessData\AUlookupTable
AUlookup <- read_csv('data/AUlookupTable/20210401_0000_AUlookup.csv')#'data/AUlookupTable/20201013_130553_AUlookup.csv')
```


```{r add in new conventionals stations}
newIRStations <- bind_rows(lastCycleStation_keepers,
                         filter(conventionals_distinct, ! (FDT_STA_ID %in% lastCycleStation_keepers$STATION_ID)) %>%
                           dplyr::select(FDT_STA_ID, Latitude, Longitude) %>%
                           rename('STATION_ID'='FDT_STA_ID', 'DD_LAT' = 'Latitude', 'DD_LONG' = 'Longitude') %>%
                           left_join(dplyr::select(AUlookup, FDT_STA_ID, ID305B_1, ID305B_2), by=c('STATION_ID' = 'FDT_STA_ID')))
  
  
```

Before we do much more work, let's reorganize the old stations table format to match the new template.

```{r match stationsTemplate}
newIRStations <- mutate(newIRStations, ID305B_4 = NA, ID305B_5 = NA, ID305B_6 = NA, ID305B_7 = NA, ID305B_8 = NA, 
               ID305B_9 = NA, ID305B_10 = NA, WATER_TYPE = NA, SALINITY = NA, 
               LACUSTRINE = ifelse(DEPTH == "LZ", 'Y', NA), # changed from "YES" to "Y" to comply with final bulk data upload data structure
               TYPE_1 = STA_TYPE1, TYPE_2 = STA_TYPE2, TYPE_3 = STA_TYPE3,
               TYPE_4 = NA, TYPE_5 = NA, TYPE_6 = NA, TYPE_7 = NA, TYPE_8 = NA, TYPE_9 = NA, TYPE_10 = NA, 
               LATITUDE = DD_LAT, LONGITUDE = DD_LONG, BENTHIC_STAT = BENTHIC_ST,
               ECOLI_GM_EXC = NA, ECOLI_GM_SAMP = NA, ENTER_GM_EXC = NA, ENTER_GM_SAMP = NA, 
               AMMONIA_EXC = NA, AMMONIA_STAT = NA, BENTHIC_WOE_CAT = NA, BIBI_SCORE = NA ) %>% # add/change columns 
  rename_at(vars(contains("WMET")), funs(str_replace(., "WMET", "WAT_MET"))) %>% # change water metals column names
  rename_at(vars(contains("WTOX")), funs(str_replace(., "WTOX", "WAT_TOX"))) %>% # change water toxics column names
  rename_at(vars(contains("SMET")), funs(str_replace(., "SMET", "SED_MET"))) %>% # change sediment metals column names
  rename_at(vars(contains("STOX")), funs(str_replace(., "STOX", "SED_TOX"))) %>% # change sediment toxics column names
  rename_at(vars(contains("FMET")), funs(str_replace(., "FMET", "FISH_MET"))) %>% # change fish metals column names
  rename_at(vars(contains("FTOX")), funs(str_replace(., "FTOX", "FISH_TOX"))) %>% # change fish toxics column names
  rename_at(vars(contains("TP_")), funs(str_replace(., "TP_", "NUT_TP_"))) %>% # change TP column names
  rename_at(vars(contains("CHLA_")), funs(str_replace(., "CHLA_", "NUT_CHLA_"))) %>% # change chl a column names
  rename_at(vars(contains("_VIO")), funs(str_replace(., "_VIO", "_EXC"))) %>% # change _VIO columns to _EXC
  dplyr::select(names(stationsTemplate)) 

# fix new stations that don't have an assessment region or VAHU6
fixed <- filter(newIRStations, is.na(VAHU6)) %>%
  st_as_sf(coords = c("LONGITUDE", "LATITUDE"), 
           remove = F, # don't remove these lat/lon cols from df
                         crs = 4326) %>% # add projection, needs to be geographic for now bc entering lat/lng
  st_intersection(vahu6) %>% 
  mutate(REGION = ASSESS_REG, VAHU6 = VAHU6.1) %>%
  dplyr::select(names(stationsTemplate)) %>%
  st_intersection(dcr11) %>% 
  mutate(WATERSHED = ANCODE) %>%
  dplyr::select(names(stationsTemplate)) %>%
  st_drop_geometry() 

# Smash together to make a new stations table for the cycle
# This data will be displayed in app to remind user of previous assessment results
stationsTable2022begin <- filter(newIRStations, ! STATION_ID %in% fixed$STATION_ID) %>%
  bind_rows(fixed)
rm(fixed)

```

Add lacustrine designation where possible (from multiple data sources). This band aid should only have to be used for 2022IR and then the new station table template should take care of this issue.

```{r}
LZ <- readRDS('data/LacustrineZoneData/lakeStationsFinal.RDS') %>%
  filter(Assess_TYPE =='LZ') %>%
  dplyr::select(FDT_STA_ID, Assess_TYPE) %>%
  rename('STATION_ID' = 'FDT_STA_ID') %>%
  # TRO: LZ designation from Aidan, email 07/19/2019
  bind_rows(
    read_excel('data/LacustrineZoneData/Lacustrine Zones.xlsx') %>%
      filter(Zone == 'Lacustrine') %>%
      rename('Assess_TYPE' = 'Zone') 
  ) %>%
  # PRO: LZ data from Kelley email on 9/30/2019
  bind_rows(
    data.frame(STATION_ID = c('2-XLW000.60', '5ARDC007.30', '2-CHK025.15', '2-CHK026.94',
                                                  '2-DSC005.91', '2-DSC007.09', '2-FAC003.85', '2-FAC005.78',
                                                  '5AGTC009.94', '5AGTC013.62', '2-SFT006.10', '2-APP020.23',
                                                  '2-APP023.27', '2-APP026.67', '2-LTL001.60', '2-LTL001.20',
                                                  '2-LTL002.46', '2-LDJ000.60', '2-STG000.21', '2-STG000.91',
                                                  '2-SFT022.14', '2-SFT031.08', '2-DYC000.19', '2-BRI010.78',
                                                  '2-BRI013.12', '2-MBN000.96', '2-SDY004.27', '2-SDY005.85', 
                                                  '2-TBM000.92', '2-XEP000.44')) %>%
      mutate(Assess_TYPE = 'Yes') ) %>%
  # VRO: LZ designations from Sara, email 10/15/2020
  bind_rows(
    read_excel('data/LacustrineZoneData/VROLacustrineStations2022RTool.xlsx') %>%
      dplyr::select(STATION_ID = `Station ID`)  ) %>% # select and rename in 1 step
  mutate(Assess_TYPE = 'Y') %>% 
  group_by(STATION_ID) %>% 
  mutate(n = n()) %>% 
  filter(n ==1) %>% 
  dplyr::select(-n)

# Fix Stations table
stationsTable2022begin <- left_join(stationsTable2022begin, LZ) %>%
  mutate(LACUSTRINE = case_when(LACUSTRINE == 'Y' | Assess_TYPE == 'Y' ~ "Y",
                                 TRUE ~ as.character(NA))) %>%
  dplyr::select(-Assess_TYPE)
```
Make sure no stations are missing AU information. Try to build tools to handle no AU information but still run with WQS information.

```{r}
nrow(filter(stationsTable2022begin, is.na(ID305B_1)))
#View(filter(stationsTable2022begin, is.na(ID305B_1)))
```

for now, maybe this is my starting point for rebuilding assessment scripts???????

```{r save}
write.csv(stationsTable2022begin, 'processedStationData/stationsTable2022begin.csv', row.names = F)
```


## Other data

### PCB Data

Mark's PCB dataset was sent out but the StationID's are messed up on quite a few samples. The following script fixes them.

```{r PCB data}
PCB <- read_excel('data/2022 IR PCBDatapull.xlsx', sheet = '2022IR Datapull')# had to manually combine date and time fields to avoid date time issues
source('PCBdataRework.R')

# be nice and save this as a sheet in dataset for others to use
write.csv(PCB, 'data/PCBfixed.csv', row.names = F) # dont write out a character NA in csv

```



### Fish Tissue Data

Fish tissue is a disaster. I ended up combining Gabe's final yearly products by hand in C:\HardDriveBackup\Assessment\IR2022\Fish Tissue\2015_2020_fish_tissue_data_WQA\FishTissuePCBsMetals_EVJ.xlsx

2020 data still needs to be hand combined. This is not a long term solution but the VIMS raw data Gabe sent does not include species information so there is a good amount of hand work going on somewhere in his methods that we need access to to join in.

```{r}
source('fishTissueCleaning.R')
```












```{r fish tissue}

fish2015 <- read_excel('data/2015_2020_fish_tissue_data_WQA/2015_fish_PCBs_v122.xlsx')
```


## Pin data to server

Once the data is organized, we need to pin data used in assessment prerequisite steps to be available to the assessors and to the assessment scripts. 

Repeat as necessary in order to access the most recent data available for assessment.

```{r pin data}
library(pins)
library(config)

# get configuration settings
conn <- config::get("connectionSettings")

# use API key to register board
board_register_rsconnect(key = conn$CONNECT_API_KEY,  #Sys.getenv("CONNECT_API_KEY"),
                          server = conn$CONNECT_SERVER)#Sys.getenv("CONNECT_SERVER"))

#conventionals2022IRdraftWithSecchi_Feb2021 <- conventionals
conventionals2022IRfinalWithSecchi <- conventionals

#conventionals2022IRdraft_Feb2021 <- conventionals
stations2020IR_sf_final <- lastCycleStation_sf
#stations2020IR_sf_draft_Feb2021 <- lastCycleStation_sf
conventionals_distinct_final <- conventionals_distinct 
#conventionals_distinct_draft_Feb2021 <- conventionals_distinct 

#2020 final metals data (pre manipulated by me) to act as a placeholder
WCmetals_2020IRfinal <- read_csv('data/final2020data/CEDSWQM_2020_IR_DATA-WATER_METALS_VALUES_20190207_EVJ.csv')
Smetals_2020IRfinal <- read_excel('data/final2020data/CEDSWQM_2020_IR_DATA-CEDSWQM_SEDIMENT_20190213.xlsx') %>%
  dplyr::select(FDT_STA_ID:ZINC...70, COMMENT...89)
names(Smetals_2020IRfinal) <- gsub( "[..].*", "", names(Smetals_2020IRfinal)) # remove anything after .. in name

# 2022 final metals data from Roger (not assessed)
WCmetals_2022IRfinal <- read_excel('data/final2022data/CEDSWQM/WATER_METALS_2022_20210201.xlsx')
Smetals_2022IRfinal <- read_excel('data/final2022data/CEDSWQM/SEDIMENT_2022_20210201.xlsx')

# Pin data
pin(conventionals2022IRfinalWithSecchi, description = "Final 2022IR conventionals dataset from Roger Stewart", board = "rsconnect")
#pin(conventionals2022IRdraftWithSecchi_Feb2021, description = "DRAFT 2022IR conventionals dataset from Roger Stewart Feb 2021 update", board = "rsconnect")

#pin(conventionals2022IRdraft_Feb2021, description = "DRAFT 2022IR conventionals dataset from Roger Stewart Feb 2021 update", board = "rsconnect")
pin(conventionals_distinct_final, description = "Final 2022IR conventionals distinct stations from Roger Stewart", board = "rsconnect")
#pin(conventionals_distinct_draft_Feb2021, description = "DRAFT 2022IR conventionals distinct stations from Roger Stewart Feb 2021 update", board = "rsconnect")
pin(vahu6, description = "DEQ Assessment Region Data at VAHU6 level", board = "rsconnect")
pin(stations2020IR_sf_final, description = "Final 2020IR stations from Cleo Baker", board = "rsconnect")
#pin(stations2020IR_sf_draft_Feb2021, description = "DRAFT 2020IR stations from Cleo Baker Feb 2021 update", board = "rsconnect")
#pin(WCmetals_2020IRfinal, description = "FINAL 2020IR water column metals data from Roger Stewart", board = "rsconnect")
#pin(Smetals_2020IRfinal, description = "FINAL 2020IR sediment metals data from Roger Stewart", board = "rsconnect")
pin(WCmetals_2022IRfinal, description = "FINAL 2022IR water column metals data from Roger Stewart", board = "rsconnect")
pin(Smetals_2022IRfinal, description = "FINAL 2022IR sediment metals data from Roger Stewart", board = "rsconnect")

```



### WQS addition

Now we need to make sure we have WQS information for each station we need to assess so that assessors only need to upload station information to the assessment applications and not bother with WQS information.

First, pull and organize any WQS information from the server. 

```{r organize WQS information}
#source('organizeWQSmetadataFromServer.R')
```

Then, take the most recent WQSlookup table and upload to server. This should be repeated once all WQS attribution is fully completed with full assessment cycle data.

```{r wqs information}
# most recent version taken from ..\1.preprocessData\WQSlookupTable
WQSlookup <- read_csv('data/WQSlookupTable/20210401_0001_WQSlookup.csv')#read_csv('data/WQSlookupTable/20201207_092616_WQSlookup.csv')

# find number of WQS_ID linked to each station
WQSlookup1 <- WQSlookup %>% 
  group_by(StationID) %>%
  summarise(distinctWQS = n_distinct(WQS_ID))


# these need to be sorted out 
moreThan1WQS <- filter(WQSlookup, StationID %in% filter(WQSlookup1, distinctWQS != 1)$StationID) %>%
  arrange(StationID)
write_csv(moreThan1WQS, 'multipleWQS.csv')

rm(moreThan1WQS); rm(WQSlookup1)



pin(WQSlookup, description = "WQS lookup table from metadata attribution application", board = "rsconnect")
```

Now read in the WQS info for the assessment and upload to server (makes syncing data across other applications better as well).

```{r}
riverine <- st_read('C:/HardDriveBackup/GIS/WQS/WQS_layers_05082020.gdb', layer = 'riverine_05082020') 
pin(riverine, description = "Riverine WQS from Tish Robertson 5/8/2020", board = "rsconnect")
rm(riverine)

lacustrine <- st_read('C:/HardDriveBackup/GIS/WQS/WQS_layers_05082020.gdb', layer = 'lakes_reservoirs_05082020')
pin(lacustrine, description = "Lacustring WQS from Tish Robertson 5/8/2020", board = "rsconnect")
rm(lacustrine)

estuarineLines <- st_read('C:/HardDriveBackup/GIS/WQS/WQS_layers_05082020.gdb', layer = 'estuarinelines_05082020')
pin(estuarineLines, description = "Estuarine Lines WQS from Tish Robertson 5/8/2020", board = "rsconnect")
rm(estuarineLines)

estuarinePolys <- st_read('C:/HardDriveBackup/GIS/WQS/WQS_layers_05082020.gdb', layer = 'estuarinepolygons_05082020') 
pin(estuarinePolys, description = "Estuarine Polys WQS from Tish Robertson 5/8/2020", board = "rsconnect")
rm(estuarinePolys)

```

To speed up the applications and assessment calculations, we will first join the actual WQS information to the lookup table now in the preprocessing steps and pin that information as a starting point for apps.

But to speed this process up, we will first pull down the data that exists on the server as to not run spatial joins more than necessary.

```{r}
WQSlookup_withStandards <- pin_get('WQSlookup-withStandards', board = "rsconnect")

# cheater step, find sites I know have issues and remove from archived version
WQStableKnownIssues <- readRDS('./data/WQStable12072020.RDS')
WQSlookup_withStandards <- filter(WQSlookup_withStandards, ! StationID %in% WQStableKnownIssues$StationID)


WQSlookupToDo <- filter(WQSlookup, ! StationID %in% WQSlookup_withStandards$StationID)

```




```{r WQS information joins, eval = FALSE}
#bring in Riverine layers, valid for the assessment window
riverine <- st_read('C:/HardDriveBackup/GIS/WQS/WQS_layers_05082020.gdb', layer = 'riverine_05082020') %>%
  st_drop_geometry() # only care about data not geometry

WQSlookupFull <- left_join(WQSlookupToDo, riverine) %>%
  filter(!is.na(CLASS))
rm(riverine)

lacustrine <- st_read('C:/HardDriveBackup/GIS/WQS/WQS_layers_05082020.gdb', layer = 'lakes_reservoirs_05082020') %>%
  st_drop_geometry() # only care about data not geometry

WQSlookupFull <- left_join(WQSlookupToDo, lacustrine) %>%
  filter(!is.na(CLASS)) %>%
  bind_rows(WQSlookupFull)
rm(lacustrine)

estuarineLines <- st_read('C:/HardDriveBackup/GIS/WQS/WQS_layers_05082020.gdb', layer = 'estuarinelines_05082020') %>%
  st_drop_geometry() # only care about data not geometry

WQSlookupFull <- left_join(WQSlookupToDo, estuarineLines) %>%
  filter(!is.na(CLASS)) %>%
  bind_rows(WQSlookupFull)
rm(estuarineLines)

estuarinePolys <- st_read('C:/HardDriveBackup/GIS/WQS/WQS_layers_05082020.gdb', layer = 'estuarinepolygons_05082020') %>%
  st_drop_geometry() # only care about data not geometry

WQSlookupFull <- left_join(WQSlookupToDo, estuarinePolys) %>%
  filter(!is.na(CLASS)) %>%
  bind_rows(WQSlookupFull)
rm(estuarinePolys)

WQSlookup_withStandards_pin <- bind_rows(WQSlookup_withStandards, WQSlookupFull) # for pin

# Find duplicates 
WQSlookup_withStandards_issues <- WQSlookup_withStandards_pin %>% 
  group_by(StationID) %>% 
  mutate(totCount = n()) %>% 
  filter(totCount > 1)
# write out and fix
write_csv(WQSlookup_withStandards_issues %>% arrange(StationID), 'data/fixMe.csv')
fixed <- read_csv('data/fixMe.csv') %>%
  mutate(GNIS_ID = as.factor(as.character(GNIS_ID)))

WQSlookup_withStandards <- filter(WQSlookup_withStandards_pin, ! StationID %in% fixed$StationID) %>%
  bind_rows(fixed)

# What stations still need WQS?
#z <- filter(WQSlookup, ! StationID %in% WQSlookupFull$StationID) %>%
#  filter(StationID %in% conventionals_distinct$FDT_STA_ID)

# fix user issues
#test <- readRDS('C:/HardDriveBackup/R/GitHub/IR2022/1.preprocessData/data/WQStable.RDS')
#y <- left_join(z, test, by = 'StationID') %>%
#  rename("WQS_ID Saved on the server" = "WQS_ID.x",
#         "WQS_ID Suggested To User" = "WQS_ID.y",
#         "Buffer Distance" = "Buffer Distance.y") %>%
#  dplyr::select(StationID, `Buffer Distance`, `WQS_ID Saved on the server`, `WQS_ID Suggested To User`, Comments)
#write.csv(y,'missingWQSforReview.csv')

```




Once I get that information back from assessors I can fully move on.




Now time to actually join standards to stations. This is going to be available on the server for users to access from the assessment scripts. The thinking right now is that it is better to store this information on the server right now and show users the data (when asked) instead of having it attached to the right of existing stationsTable (like 2020 cycle) since CEDS WQA only accepts a specific template. Also, if WQS info needs to be changed, the assessor is forced to update the WQS_ID. Maybe this isn't an awesome method but going with it for now.

```{r pin WQS information}
pin(WQSlookup_withStandards, description = "WQS lookup table with Standards from metadata attribution application", board = "rsconnect")
```

Lots of citizen, non agency sites without WQS info. Need to figure out a solution to these sites.


