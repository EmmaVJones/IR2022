---
title: "Background"
author: "Emma Jones"
date: "7/12/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)


httr::set_config(httr::config(ssl_verifypeer = FALSE, ssl_verifyhost = FALSE))

library(tidyverse)
library(config)
library(sf)
library(plotly)
library(lubridate)
library(pool)
library(pins)
library(sqldf)
library(dbplyr)


# Server connection things
conn <- config::get("connectionSettings") # get configuration settings


board_register_rsconnect(key = conn$CONNECT_API_KEY,  #Sys.getenv("CONNECT_API_KEY"),
                         server = conn$CONNECT_SERVER)#Sys.getenv("CONNECT_SERVER"))

## For testing: connect to ODS production
pool <- dbPool(
 drv = odbc::odbc(),
 Driver = "ODBC Driver 11 for SQL Server",#"SQL Server Native Client 11.0",
 Server= "DEQ-SQLODS-PROD,50000",
 dbname = "ODS",
 trusted_connection = "yes"
)
```

This project holds scripts to build an on the fly automated assessment program. There will be an application interface where users can select stations and data windows to run and explore interactively and another reporting method that will simplify results into a report emailed to users based on requested station and window info.

The process first queries "conventionals" data from CEDS ODS, then it runs the available automated assessment methods against the station data and summarizes it in a "stations table" format. Other functions can break this down into useable information for various user groups.

Below outlines the method that will be migrated into an application format.

```{r automated assessment needs}
# bring in methods and data for automated assessment to work
source('methods/conventionalsFunction.R')
source('methods/updatedBacteriaCriteria.R')
source('methods/3.automatedAssessment_global.R')
source('methods/automatedAssessmentFunctions.R')
source('methods/assessmentFunction.R')
lakeNutStandards <- read_csv('data/9VAC25-260-187lakeNutrientStandards.csv')

```



1) Identify station(s) and temporal window.

```{r identify stations}

station <- pin_get("conventionals2022IRfinalWithSecchi", board = "rsconnect") %>% 
  filter(STA_REC_CODE == 'BRRO') %>% 
  filter(str_detect(FDT_STA_ID, '2-JKS') ) %>% 
  distinct(FDT_STA_ID) %>% 
  pull()

dateRange <- c(as.Date('2020-01-01'), as.Date(Sys.Date()))

```



2) Pull corresponding field and analyte data. If too many records requested, run multiple queries.


```{r pull data}
stationFieldData <- pool %>% tbl(in_schema("wqm", "Wqm_Field_Data_View")) %>%
  filter(Fdt_Sta_Id %in% !! station &
           between(as.Date(Fdt_Date_Time), !! dateRange[1], !! dateRange[2]) ) %>%
    filter(! Ssc_Description %in% "INVALID DATA SET QUALITY ASSURANCE FAILURE") %>% 
  as_tibble()

stationAnalyteData <- pool %>% tbl(in_schema("wqm", "Wqm_Analytes_View")) %>%
  filter(Ana_Sam_Fdt_Id %in% !! stationFieldData$Fdt_Id &
           #between(as.Date(Ana_Received_Date), !! dateRange[1], !! dateRange[2]) & # x >= left & x <= right
           Pg_Parm_Name != "STORET STORAGE TRANSACTION DATE YR/MO/DAY") %>%
  as_tibble() %>%
  left_join(dplyr::select(stationFieldData, Fdt_Id, Fdt_Sta_Id, Fdt_Date_Time), by = c("Ana_Sam_Fdt_Id" = "Fdt_Id"))
stationInfo <- pool %>% tbl(in_schema("wqm",  "Wqm_Stations_View")) %>%
  filter(Sta_Id %in% !! toupper(station)) %>%
  as_tibble()
stationGIS_View <-  pool %>% tbl(in_schema("wqm",  "Wqm_Sta_GIS_View")) %>%
  filter(Station_Id %in% !! toupper(station)) %>%
  as_tibble()
conventionals <- conventionalsSummary(conventionals= pin_get("conventionals2022IRfinalWithSecchi", board = "rsconnect")[0,],
                           stationFieldDataUserFilter= stationFieldData, stationAnalyteDataUserFilter = stationAnalyteData,
                           stationInfo,
                           stationGIS_View,
                           dropCodes = c('QF'))%>% 
  arrange(FDT_STA_ID, FDT_DATE_TIME, FDT_DEPTH) #%>% 
```

3) Organize metadata. Keep this separate so users can see information and adjust stations to be lake/lacustrine as needed.

```{r metadata}
WQSlookup <- pin_get("WQSlookup-withStandards",  board = "rsconnect")
WQMstationSpatial <- pin_get("WQM-Stations-Spatial", board = "rsconnect")
VSCIresults <- pin_get("VSCIresults", board = "rsconnect") %>%
  filter( between(`Collection Date`, dateRange[1], dateRange[2]) )

stationTable <- left_join(tibble(STATION_ID = station),
                          WQSlookup, by = c('STATION_ID'='StationID')) %>%
  mutate(CLASS_BASIN = paste(CLASS,substr(BASIN, 1,1), sep="_")) %>%
  mutate(CLASS_BASIN = ifelse(CLASS_BASIN == 'II_7', "II_7", as.character(CLASS))) %>%
  # Fix for Class II Tidal Waters in Chesapeake (bc complicated DO/temp/etc standard)
  left_join(WQSvalues, by = 'CLASS_BASIN') %>%
  dplyr::select(-c(CLASS.y,CLASS_BASIN)) %>%
  rename('CLASS' = 'CLASS.x') %>%
  left_join(WQMstationSpatial %>% distinct(StationID, .keep_all = TRUE), by = c('STATION_ID' = 'StationID')) %>%
  # last cycle had code to fix Class II Tidal Waters in Chesapeake (bc complicated DO/temp/etc standard) but not sure if necessary
  lakeNameStandardization() %>% # standardize lake names
  
   
  # extra special step
  mutate(Lake_Name = case_when(STATION_ID %in% c('2-TRH000.40') ~ 'Thrashers Creek Reservoir',
                               STATION_ID %in% c('2-LSL000.16') ~ 'Lone Star Lake F (Crystal Lake)',
                               STATION_ID %in% c('2-LSL000.04') ~ 'Lone Star Lake G (Crane Lake)',
                               STATION_ID %in% c('2-LSL000.20') ~ 'Lone Star Lake I (Butler Lake)',
                               STATION_ID %in% c('2-NWB002.93','2-NWB004.67', '2-NWB006.06') ~ 'Western Branch Reservoir',
                               STATION_ID %in% c('2-LDJ000.60') ~ 'Lake Nottoway (Lee Lake)',
                               TRUE ~ as.character(Lake_Name))) %>%
  left_join(lakeNutStandards %>% 
              mutate(Lakes_187B = 'y'),  # special step to make sure the WQS designation for 187 are correct even when not
            by = c('Lake_Name')) %>%
  # lake drummond special standards
  mutate(Lakes_187B = ifelse(is.na(Lakes_187B.y ), Lakes_187B.x, Lakes_187B.y), 
    `Chlorophyll a (ug/L)` = case_when(Lake_Name %in% c('Lake Drummond') ~ 35,
                                            TRUE ~ as.numeric(`Chlorophyll a (ug/L)`)),
         `Total Phosphorus (ug/L)` = case_when(Lake_Name %in% c('Lake Drummond') ~ 40,
                                               TRUE ~ as.numeric(`Total Phosphorus (ug/L)`))) %>% 
  dplyr::select(STATION_ID:StreamType, Lakes_187B, `Description Of Waters`:`Total Phosphorus (ug/L)`)


# stationTableForUserReview <- dplyr::select(stationTable, STATION_ID, SEC:Lakes_187B) %>% 
#   mutate(`Lake Station` = ifelse(Lakes_187B == 'y', T, F),
#          `Lacustrine Zone` = F,
#          lake1= paste0('<input type="checkbox"  name=\"Dec\" value="%s"/>') )
# 
# sprintf(
#       '<input type="checkbox" name="%s" value="%s"/>',
#       #        '<input type="radio" name="%s" value="%s"/>',
#       month.abb[i], m[i, ]
#     )


# Need to find way to identify lake stations
# # Traditional assessment method won't work
# lakeStations <- filter(stationTable, str_detect(ID305B_1, 'L_') | str_detect(ID305B_2, 'L_') | str_detect(ID305B_3, 'L_') | str_detect(ID305B_4, 'L_') |
#                            str_detect(ID305B_5, 'L_') | str_detect(ID305B_6, 'L_') | str_detect(ID305B_7, 'L_') | str_detect(ID305B_8, 'L_') |
#                            str_detect(ID305B_9, 'L_') | str_detect(ID305B_10, 'L_') )

# using just a vector of names for now, ideally will be sourced by providing and DT::datatable with checkbox field and using the checkbox information to
# identify whether a station is a lake station or not
lakeStations <- filter(stationTable, STATION_ID %in% c('2-JKS044.60', '2-JKS046.40', '2-JKS048.90', '2-JKS053.48'))
lacustrineDesignation <- filter(stationTable, STATION_ID %in% c('2-JKS044.60', '2-JKS046.40'))
```



3) Assess data

```{r automated assessment function}
automatedAssessmentFunction <- function(stationTable, conventionals, lakeStations, lacustrineDesignation, VSCIresults){
  stationTableResults <- tibble()
# save ammonia results (based on default assessment information) for use in app to speed rendering
ammoniaAnalysis <- tibble()

for(i in 1:nrow(stationTable)){
#i = 1
  print(paste('Assessing station', i, 'of', nrow(stationTable), sep=' '))

  # pull one station data
  stationData <- filter(conventionals, FDT_STA_ID %in% stationTable$STATION_ID[i]) %>%
    left_join(stationTable, by = c('FDT_STA_ID' = 'STATION_ID')) %>%
    pHSpecialStandardsCorrection() %>% # correct pH to special standards where necessary
    # special lake steps
    {if(stationTable$STATION_ID[i] %in% lakeStations$STATION_ID)
      suppressWarnings(suppressMessages(
        mutate(., lakeStation = TRUE) %>%
        thermoclineDepth())) # adds thermocline information and SampleDate
      else mutate(., lakeStation = FALSE) } %>% 
    # manually add lacustrine designation per user input
    mutate(LACUSTRINE = ifelse(stationTable$STATION_ID[i] %in% lacustrineDesignation$STATION_ID, TRUE, FALSE))
   

  # If data exists for station, run it
  if(nrow(stationData) > 0){
    # Date last sampled
    dateLastSampled <- as.character(max(stationData$FDT_DATE_TIME)) } else {dateLastSampled <- 'No data in conventionals data pull'}

   
  # Ammonia special section
  ammoniaAnalysisStation <- freshwaterNH3limit(stationData, trout = ifelse(unique(stationData$CLASS) %in% c('V','VI'), TRUE, FALSE),
                                        mussels = TRUE, earlyLife = TRUE) 
  # https://law.lis.virginia.gov/admincode/title9/agency25/chapter260/section155/ states the assumption is that
  #  waters are to be assessed with the assumption that mussels and early life stages of fish should be present
  # trout presence is determined by WQS class, this can be changed in the app but is forced to be what the station
  # is attributed to in the automated assessment scripts
  
     
  if(nrow(stationData) > 0){
    # PWS stuff
    if(is.na(unique(stationData$PWS))  ){
      PWSconcat <- tibble(#STATION_ID = unique(stationData$FDT_STA_ID),
                          PWS= NA)
   } else {
     PWSconcat <- cbind(#tibble(STATION_ID = unique(stationData$FDT_STA_ID)),
                         assessPWS(stationData, NITRATE_mg_L, LEVEL_NITRATE, 10, 'PWS_Nitrate'),
                         assessPWS(stationData, CHLORIDE_mg_L, LEVEL_CHLORIDE, 250, 'PWS_Chloride'),
                         assessPWS(stationData, SULFATE_TOTAL_mg_L, LEVEL_SULFATE_TOTAL, 250, 'PWS_Total_Sulfate')) %>%
       dplyr::select(-ends_with('exceedanceRate')) }
    
  # chloride assessment if data exists
    if(nrow(filter(stationData, !is.na(CHLORIDE_mg_L))) > 0){
      chlorideFreshwater <- chlorideFreshwaterSummary(suppressMessages(chlorideFreshwaterAnalysis(stationData)))
    } else {chlorideFreshwater <- tibble(CHL_EXC = NA, CHL_STAT= NA)}
    
  # Nutrients based on station type
    # Nutrient: TP (lakes have real standards; riverine no longer uses 0.2 mg/L as an observed effect for Aquatic life use but will use it as flag for this)
    if(unique(stationData$lakeStation) == TRUE){
      TP <- TP_Assessment(stationData) 
      } else {
        TP <- countNutrients(stationData, PHOSPHORUS_mg_L, LEVEL_PHOSPHORUS, 0.2) %>% quickStats('NUT_TP') %>%   # using 0.2mg/L as flag for this "assessment"
          mutate(NUT_TP_STAT = ifelse(NUT_TP_STAT != "S", "Review", NA)) } # flag OE but don't show a real assessment decision
    
      # Nutrients: Chl a (lakes)
    if(unique(stationData$lakeStation) == TRUE){
      chla <- chlA_Assessment(stationData)
        #tibble(NUT_CHLA_EXC = NA, NUT_CHLA_SAMP = NA, NUT_CHLA_STAT = NA) # placeholder for now
      } else {
        chla <- countNutrients(stationData, CHLOROPHYLL_A_ug_L, LEVEL_CHLOROPHYLL_A, NA) %>% quickStats('NUT_CHLA') %>%
          mutate(NUT_CHLA_STAT = NA) } # don't show a real assessment decision
    
    # run DO daily average status for riverine and tuck results into comments later
    if(unique(stationData$lakeStation) == TRUE){
      DO_Daily_Avg_STAT <- ''
    } else {
      DO_Daily_Avg_STAT <- paste0('DO_Daily_Avg_STAT: ', 
                                       DO_Assessment_DailyAvg(stationData) %>% 
                                         quickStats('DO_Daily_Avg') %>% 
                                         dplyr::select(DO_Daily_Avg_STAT) %>% pull())}
  
  
  results <- cbind(
      dplyr::select(stationData, STATION_ID = FDT_STA_ID, Sta_Desc:BASIN_CODE, Basin_Code= Basin_Code.y, CountyCityName:Location) %>% 
        distinct(STATION_ID, .keep_all = T),
      tempExceedances(stationData) %>% quickStats('TEMP'),
      DOExceedances_Min(stationData) %>% quickStats('DO'), 
      # this will be removed for lake stations later since it does not apply
      pHExceedances(stationData) %>% quickStats('PH'),
      bacteriaAssessmentDecisionClass(stationData),
      ammoniaDecision(list(acute = freshwaterNH3Assessment(ammoniaAnalysisStation, 'acute'),
                           chronic = freshwaterNH3Assessment(ammoniaAnalysisStation, 'chronic'),
                           fourDay = freshwaterNH3Assessment(ammoniaAnalysisStation, 'four-day'))), 
    
      # Will add in metals assessment at a later date
      
      # PCB and fish info can only be incorporated when data migrated into CEDS
      
      # Benthics, just a flag that benthic data exists
      benthicAssessment(stationData, VSCIresults),
      
      # Nutrient Assessment done above by waterbody type
      TP,
      chla) %>%
    # COMMENTS
      mutate(COMMENTS = paste0(DO_Daily_Avg_STAT, 
                               ' | AMMONIA Comment: ', `Assessment Decision`,
            BACTERIA_COMMENTS = paste0(' | E.coli Comment: ', ECOLI_STATECOLI_VERBOSE,
                              ' | Enterococci Comment: ', ENTER_STATENTER_VERBOSE)) ) %>%
      dplyr::select(-ends_with(c('exceedanceRate', 'VERBOSE', 'Assessment Decision', 'StationID'))) %>%  # to match Bulk Upload template but helpful to keep visible til now for testing
      mutate(`Date Last Sampled` = dateLastSampled) 
  } else {# pull what you can from last cycle and flag as carry over
    results <- filter(stationTable, STATION_ID == stationTable$STATION_ID[i]) %>%
      dplyr::select(STATION_ID , Sta_Desc:BASIN_CODE, Basin_Code= Basin_Code.y, CountyCityName:Location) %>% 
        distinct(STATION_ID, .keep_all = T) %>% 
      mutate(COMMENTS = 'This station has no data in current window.') %>%
      mutate(`Date Last Sampled` = dateLastSampled)
  }
    stationTableResults <- bind_rows(stationTableResults, results) %>% 
      relocate(c( BACTERIADECISION, BACTERIASTATS), .after = `Date Last Sampled`)
  ammoniaAnalysis <- bind_rows(ammoniaAnalysis, tibble(StationID = unique(stationData$FDT_STA_ID), AmmoniaAnalysis = list(ammoniaAnalysisStation)))
}

return(list(stationTableResults = stationTableResults, ammoniaAnalysis = ammoniaAnalysis))
}

```


How to run assessment

```{r run assessment}
z <- automatedAssessmentFunction(stationTable, conventionals)
z$stationTableResults
z$ammoniaAnalysis

```



### Practice exercise to get stations sampled by a region from beginning of year.


Try going from people to data 

```{r BRRO since start of year practice}
dateRange <- c(as.Date('2021-01-01'), as.Date(Sys.Date()))
region <- c('SCRO', 'WCRO')# convert BRRO to codes manually until fixed
spgCodes <- NULL#c('IM', 'TM')

if(!is.null(region)){
  peeps <- pool %>% tbl(in_schema("wqm", "Wqm_Collector_Cds_Codes_Wqm_View")) %>% 
  filter(Unt_Rec_Code %in% !! region) %>% 
  as_tibble()
  } else {peeps <- tibble()}


stationFieldData <- pool %>% tbl(in_schema("wqm", "Wqm_Field_Data_View")) %>%
  filter(between(as.Date(Fdt_Date_Time), !! dateRange[1], !! dateRange[2]) ) %>% # date range always required
  # if Region (aka people) selected
  {if(nrow(peeps) > 0)
    filter(., Fdt_Collector_Id %in% !! peeps$Col_Id )
    else . } %>%
  # if Program Code selected
  {if(!is.null(spgCodes))
    filter(., Fdt_Spg_Code %in% !! spgCodes)
    else . } %>% 
  # always drop crap data
  filter(! Ssc_Description %in% "INVALID DATA SET QUALITY ASSURANCE FAILURE") %>% 
  as_tibble() 

# still need this for other functions
station <- unique(stationFieldData$Fdt_Sta_Id)
           
stationAnalyteData <- pool %>% tbl(in_schema("wqm", "Wqm_Analytes_View")) %>%
  filter(Ana_Sam_Fdt_Id %in% !! stationFieldData$Fdt_Id &
           #between(as.Date(Ana_Received_Date), !! dateRange[1], !! dateRange[2]) & # x >= left & x <= right
           Pg_Parm_Name != "STORET STORAGE TRANSACTION DATE YR/MO/DAY") %>%
  as_tibble() %>%
  left_join(dplyr::select(stationFieldData, Fdt_Id, Fdt_Sta_Id, Fdt_Date_Time), by = c("Ana_Sam_Fdt_Id" = "Fdt_Id"))

stationInfo <- pool %>% tbl(in_schema("wqm",  "Wqm_Stations_View")) %>%
  filter(Sta_Id %in% !! toupper(station)) %>%
  as_tibble()

stationGIS_View <-  pool %>% tbl(in_schema("wqm",  "Wqm_Sta_GIS_View")) %>%
  filter(Station_Id %in% !! toupper(station)) %>%
  as_tibble()

conventionals <- conventionalsSummary(conventionals= pin_get("conventionals2022IRfinalWithSecchi", board = "rsconnect")[0,],
                           stationFieldDataUserFilter= stationFieldData, stationAnalyteDataUserFilter = stationAnalyteData,
                           stationInfo,
                           stationGIS_View,
                           dropCodes = c('QF'))%>% 
  arrange(FDT_STA_ID, FDT_DATE_TIME, FDT_DEPTH) #%>% 

WQSlookup <- pin_get("WQSlookup-withStandards",  board = "rsconnect")
WQMstationSpatial <- pin_get("WQM-Stations-Spatial", board = "rsconnect")
VSCIresults <- pin_get("VSCIresults", board = "rsconnect") %>%
  filter( between(`Collection Date`, dateRange[1], dateRange[2]) )


stationTable <- left_join(tibble(STATION_ID = station),
                          WQSlookup, by = c('STATION_ID'='StationID')) %>%
  mutate(CLASS_BASIN = paste(CLASS,substr(BASIN, 1,1), sep="_")) %>%
  mutate(CLASS_BASIN = ifelse(CLASS_BASIN == 'II_7', "II_7", as.character(CLASS))) %>%
  # Fix for Class II Tidal Waters in Chesapeake (bc complicated DO/temp/etc standard)
  left_join(WQSvalues, by = 'CLASS_BASIN') %>%
  dplyr::select(-c(CLASS.y,CLASS_BASIN)) %>%
  rename('CLASS' = 'CLASS.x') %>%
  left_join(WQMstationSpatial %>% distinct(StationID, .keep_all = TRUE), by = c('STATION_ID' = 'StationID')) %>%
  # last cycle had code to fix Class II Tidal Waters in Chesapeake (bc complicated DO/temp/etc standard) but not sure if necessary
  lakeNameStandardization() %>% # standardize lake names
  
   
  # extra special step
  mutate(Lake_Name = case_when(STATION_ID %in% c('2-TRH000.40') ~ 'Thrashers Creek Reservoir',
                               STATION_ID %in% c('2-LSL000.16') ~ 'Lone Star Lake F (Crystal Lake)',
                               STATION_ID %in% c('2-LSL000.04') ~ 'Lone Star Lake G (Crane Lake)',
                               STATION_ID %in% c('2-LSL000.20') ~ 'Lone Star Lake I (Butler Lake)',
                               STATION_ID %in% c('2-NWB002.93','2-NWB004.67', '2-NWB006.06') ~ 'Western Branch Reservoir',
                               STATION_ID %in% c('2-LDJ000.60') ~ 'Lake Nottoway (Lee Lake)',
                               TRUE ~ as.character(Lake_Name))) %>%
  left_join(lakeNutStandards %>% 
              mutate(Lakes_187B = 'y'),  # special step to make sure the WQS designation for 187 are correct even when not
            by = c('Lake_Name')) %>%
  # lake drummond special standards
  mutate(Lakes_187B = ifelse(is.na(Lakes_187B.y ), Lakes_187B.x, Lakes_187B.y), 
    `Chlorophyll a (ug/L)` = case_when(Lake_Name %in% c('Lake Drummond') ~ 35,
                                            TRUE ~ as.numeric(`Chlorophyll a (ug/L)`)),
         `Total Phosphorus (ug/L)` = case_when(Lake_Name %in% c('Lake Drummond') ~ 40,
                                               TRUE ~ as.numeric(`Total Phosphorus (ug/L)`))) %>% 
  dplyr::select(STATION_ID:StreamType, Lakes_187B, `Description Of Waters`:`Total Phosphorus (ug/L)`)

# using just a vector of names for now, ideally will be sourced by providing and DT::datatable with checkbox field and using the checkbox information to
# identify whether a station is a lake station or not
lakeStations <- filter(stationTable, STATION_ID %in% c('2-JKS044.60', '2-JKS046.40', '2-JKS048.90', '2-JKS053.48'))
lacustrineDesignation <- filter(stationTable, STATION_ID %in% c('2-JKS044.60', '2-JKS046.40'))

# run assessment
# using just a vector of names for now, ideally will be sourced by providing and DT::datatable with checkbox field and using the checkbox information to
# identify whether a station is a lake station or not
lakeStations <- filter(stationTable, STATION_ID %in% c('2-JKS044.60', '2-JKS046.40', '2-JKS048.90', '2-JKS053.48'))
lacustrineDesignation <- filter(stationTable, STATION_ID %in% c('2-JKS044.60', '2-JKS046.40'))

assessmentResults <- automatedAssessmentFunction(stationTable, conventionals, 
                                                 lakeStations = filter(stationTable, Lakes_187B == 'y' ), 
                                                 lacustrineDesignation, 
                                                 VSCIresults)
View(assessmentResults$stationTableResults)
assessmentResults$ammoniaAnalysis


```




# Monitoring Report Workflow

This section identifies efficient workflows to stay on top of data collection and QA.

Ways to run report:
- by region
- by collector
- by program type

Basic report by region.

```{r query by Region}
dateRange <- c(as.Date('2021-01-01'), as.Date(Sys.Date()))
region <- c('SCRO', 'WCRO')# convert BRRO to codes manually until fixed
spgCodes <- NULL#c('IM', 'TM')

if(!is.null(region)){
  peeps <- pool %>% tbl(in_schema("wqm", "Wqm_Collector_Cds_Codes_Wqm_View")) %>% 
    {if(any(region %in% c('SCRO', 'WCRO')))
      filter(., Unt_Rec_Code %in% !! region | Col_Id == 'RJS') 
      else filter(., Unt_Rec_Code) } %>% 
    as_tibble()
  } else {peeps <- tibble()}


stationFieldData <- pool %>% tbl(in_schema("wqm", "Wqm_Field_Data_View")) %>%
  filter(between(as.Date(Fdt_Date_Time), !! dateRange[1], !! dateRange[2]) ) %>% # date range always required
  # if Region (aka people) selected
  {if(nrow(peeps) > 0)
    filter(., Fdt_Collector_Id %in% !! peeps$Col_Id )
    else . } %>%
  # if Program Code selected
  {if(!is.null(spgCodes))
    filter(., Fdt_Spg_Code %in% !! spgCodes)
    else . } %>% 
  # always drop crap data
  filter(! Ssc_Description %in% "INVALID DATA SET QUALITY ASSURANCE FAILURE") %>% 
  as_tibble() 

# still need this for other functions
station <- unique(stationFieldData$Fdt_Sta_Id)
           
stationAnalyteData <- pool %>% tbl(in_schema("wqm", "Wqm_Analytes_View")) %>%
  filter(Ana_Sam_Fdt_Id %in% !! stationFieldData$Fdt_Id &
           #between(as.Date(Ana_Received_Date), !! dateRange[1], !! dateRange[2]) & # x >= left & x <= right
           Pg_Parm_Name != "STORET STORAGE TRANSACTION DATE YR/MO/DAY") %>%
  as_tibble() %>%
  left_join(dplyr::select(stationFieldData, Fdt_Id, Fdt_Sta_Id, Fdt_Date_Time), by = c("Ana_Sam_Fdt_Id" = "Fdt_Id"))

stationInfo <- pool %>% tbl(in_schema("wqm",  "Wqm_Stations_View")) %>%
  filter(Sta_Id %in% !! toupper(station)) %>%
  as_tibble()

stationGIS_View <-  pool %>% tbl(in_schema("wqm",  "Wqm_Sta_GIS_View")) %>%
  filter(Station_Id %in% !! toupper(station)) %>%
  as_tibble()


WQSlookup <- pin_get("WQSlookup-withStandards",  board = "rsconnect")
WQMstationSpatial <- pin_get("WQM-Stations-Spatial", board = "rsconnect")
VSCIresults <- pin_get("VSCIresults", board = "rsconnect") %>%
  filter( between(`Collection Date`, dateRange[1], dateRange[2]) )


stationTable <- left_join(tibble(STATION_ID = station),
                          WQSlookup, by = c('STATION_ID'='StationID')) %>%
  mutate(CLASS_BASIN = paste(CLASS,substr(BASIN, 1,1), sep="_")) %>%
  mutate(CLASS_BASIN = ifelse(CLASS_BASIN == 'II_7', "II_7", as.character(CLASS))) %>%
  # Fix for Class II Tidal Waters in Chesapeake (bc complicated DO/temp/etc standard)
  left_join(WQSvalues, by = 'CLASS_BASIN') %>%
  dplyr::select(-c(CLASS.y,CLASS_BASIN)) %>%
  rename('CLASS' = 'CLASS.x') %>%
  left_join(WQMstationSpatial %>% distinct(StationID, .keep_all = TRUE), by = c('STATION_ID' = 'StationID')) %>%
  # last cycle had code to fix Class II Tidal Waters in Chesapeake (bc complicated DO/temp/etc standard) but not sure if necessary
  lakeNameStandardization() %>% # standardize lake names
  
   
  # extra special step
  mutate(Lake_Name = case_when(STATION_ID %in% c('2-TRH000.40') ~ 'Thrashers Creek Reservoir',
                               STATION_ID %in% c('2-LSL000.16') ~ 'Lone Star Lake F (Crystal Lake)',
                               STATION_ID %in% c('2-LSL000.04') ~ 'Lone Star Lake G (Crane Lake)',
                               STATION_ID %in% c('2-LSL000.20') ~ 'Lone Star Lake I (Butler Lake)',
                               STATION_ID %in% c('2-NWB002.93','2-NWB004.67', '2-NWB006.06') ~ 'Western Branch Reservoir',
                               STATION_ID %in% c('2-LDJ000.60') ~ 'Lake Nottoway (Lee Lake)',
                               TRUE ~ as.character(Lake_Name))) %>%
  left_join(lakeNutStandards %>% 
              mutate(Lakes_187B = 'y'),  # special step to make sure the WQS designation for 187 are correct even when not
            by = c('Lake_Name')) %>%
  # lake drummond special standards
  mutate(Lakes_187B = ifelse(is.na(Lakes_187B.y ), Lakes_187B.x, Lakes_187B.y), 
    `Chlorophyll a (ug/L)` = case_when(Lake_Name %in% c('Lake Drummond') ~ 35,
                                            TRUE ~ as.numeric(`Chlorophyll a (ug/L)`)),
         `Total Phosphorus (ug/L)` = case_when(Lake_Name %in% c('Lake Drummond') ~ 40,
                                               TRUE ~ as.numeric(`Total Phosphorus (ug/L)`))) %>% 
  dplyr::select(STATION_ID:StreamType, Lakes_187B, `Description Of Waters`:`Total Phosphorus (ug/L)`)



conventionals <- conventionalsSummary(conventionals= pin_get("conventionals2022IRfinalWithSecchi", board = "rsconnect")[0,],
                           stationFieldDataUserFilter= stationFieldData, stationAnalyteDataUserFilter = stationAnalyteData,
                           stationInfo,
                           stationGIS_View,
                           dropCodes = c('QF'))%>% 
  arrange(FDT_STA_ID, FDT_DATE_TIME, FDT_DEPTH) #%>% 

# run assessment
# identify whether a station is a lake station or not

assessmentStations <-  pool %>% tbl(in_schema("wqa", "Wqa_Station_Details_View")) %>% 
  filter(STA_NAME %in% !! station) %>% 
  as_tibble() %>% 
  group_by(STA_NAME) %>% 
  filter(WSD_CYCLE == max(WSD_CYCLE))

lakeStations <- pool %>% tbl(in_schema("wqa", "WQA_Station_Types_View")) %>% 
  filter(STX_STATION_DETAIL_ID %in% !! assessmentStations$WXA_STATION_DETAIL_ID && STL_TYPE_CODE == 'L') %>% 
  as_tibble() %>% 
  left_join(assessmentStations, by = c('STX_STATION_DETAIL_ID' = 'WXA_STATION_DETAIL_ID')) %>% 
  distinct(WSD_STATION_ID) %>% 
  pull()

lacustrineDesignation <- filter(assessmentStations, WSD_STATION_ID %in% lakeStations &&  WSD_LAC_ZONE_YN == 'Y') %>% 
  distinct( WSD_STATION_ID) %>% 
  pull()

assessmentResults <- automatedAssessmentFunction(stationTable, conventionals, 
                                                 lakeStations = filter(stationTable, STATION_ID %in% lakeStations ), 
                                                 lacustrineDesignation = filter(stationTable, STATION_ID %in% lacustrineDesignation), 
                                                 VSCIresults)
View(assessmentResults$stationTableResults)

```


Summarize this information for user.

```{r regional summary}

summarizeRuns <- function(stationFieldData){
  stationFieldData %>% 
    mutate(SampleDate = as.Date(Fdt_Date_Time)) %>% 
    group_by(Fdt_Collector_Id, Fdt_Run_Id) %>% 
    summarise(`Times Completed` = length(unique(SampleDate)),
              `Dates Sampled` = paste0(unique(SampleDate), collapse = ', '),
              `Stations Per Run` = length(unique(Fdt_Sta_Id))) }
z <- summarizeRuns(stationFieldData)

# Run and Station summary by monitor, this should be reported first, then dig in to summarizeRun info by person
z %>% 
  group_by(Fdt_Collector_Id) %>% 
  summarise(`Runs Completed` = sum(`Times Completed`),
            `Stations Monitored` = sum(`Stations Per Run`))
```

```{r collector Heatmap}
library(inlmisc)
library(leaflet)
assessmentLayer <- st_read('data/GIS/AssessmentRegions_VA84_basins.shp') %>%
  st_transform( st_crs(4326))
collectorID <- 'RJS'
collectorHeatmap <- function(stationFieldData, stationGIS_View, assessmentLayer, collectorID){
  stations <- filter(stationFieldData, Fdt_Collector_Id %in% collectorID)
  if(nrow(stations) > 0){
    stations <- stations %>% 
      group_by(Fdt_Sta_Id) %>% 
    summarise(`Station Visited` = length(unique(Fdt_Date_Time))) %>% 
    left_join(stationGIS_View, by = c('Fdt_Sta_Id' = 'Station_Id')) %>% 
    st_as_sf(coords = c("Longitude", "Latitude"),  # make spatial layer using these columns
                   remove = F, # don't remove these lat/lon cols from df
                   crs = 4326)
  
    VAHU6 <- filter(assessmentLayer, VAHU6 %in% stations$Huc6_Vahu6)
  
  pal <- colorNumeric(
    palette = "Reds",
    domain = stations$`Station Visited`)
  
  CreateWebMap(maps = c("Topo","Imagery","Hydrography"), collapsed = TRUE,
                 options= leafletOptions(zoomControl = TRUE,minZoom = 3, maxZoom = 20,
                                         preferCanvas = TRUE)) %>%
      #setView(-79.1, 37.7, zoom=7)  %>%
      addPolygons(data= VAHU6,  color = 'gray', weight = 1,
                  fillColor= 'yellow', fillOpacity = 0.5,stroke=0.1,
                  group="VAHU6 sampled",label = ~VAHU6) %>% 
    addCircleMarkers(data = stations, color = 'black', fillColor = ~pal(stations$`Station Visited`),
                     radius = 3, fillOpacity = 1, weight = 1,stroke=T, label = ~Fdt_Sta_Id,
                     popup = leafpop::popupTable(stations, zcol=c('Fdt_Sta_Id', 'Station Visited'))) %>% 
    addLegend(data = stations,'topright', pal = pal, values = ~`Station Visited`, title = 'Number of Station Visits')

  
  } else { return(NULL)}
}


collectorHeatmap(stationFieldData, stationGIS_View, assessmentLayer, collectorID = 'RJS')#'SMH')
```



Dig into exceedances

```{r}
# highlight stations where exceedances occur
View(filter_at(assessmentResults$stationTableResults, vars(ends_with("_EXC")), any_vars( . > 0)))
               #vars(c('TEMP_STAT', 'DO_STAT')), any_vars(. %in%  c('10.5% Exceedance', 'IM')) ))
               #vars(ends_with("_STAT")), any_vars(. %in%  c('Review', '10.5% Exceedance', 'IM')) ))
```

```{r}
# parameter name crosswalk for prettier names later
parameterEXCcrosswalk <- tibble(Parameter = c("Temperature", 'Dissolved Oxygen', 'pH', 'E.coli STV', 'E.coli Geomean', 'Enterococci STV',  'Enterococci Geomean',
                                              'Ammonia', 'Water Column Metals',
                                               'Water Column Toxics', 'Sediment Metals', 'Sediment Toxics', 'Fish Tissue Metals', 'Fish Tissue Toxics',
                                               'Benthics', 'Total Phosphorus', 'Chlorophyll a'),
                                 ParameterEXC = c("TEMP_EXC", "DO_EXC", "PH_EXC", "ECOLI_EXC", "ECOLI_GM_EXC ","ENTER_EXC", "ENTER_GM_EXC",
                                                  "AMMONIA_EXC", "WAT_MET_EXC", 
                                                   "WAT_TOX_EXC", "SED_MET_EXC", "SED_TOX_EXC", "FISH_MET_EXC", "FISH_TOX_EXC", "BENTHIC_EXC",
                                                   "NUT_TP_EXC", "NUT_CHLA_EXC"))


stationTableResults <- left_join(assessmentResults$stationTableResults,
                                 dplyr::select(stationGIS_View, STATION_ID = Station_Id, LATITUDE = Latitude, LONGITUDE = Longitude),
                                 by = 'STATION_ID')


stationSummary <- function(stationTableResults, parameterSTATcrosswalk){
  #vahu6StationSummary <- filter(stationTable, VAHU6 %in% VAHU6chosen) 
  
  
  if(nrow(stationTableResults) > 0){
    stationTableResults1 <- stationTableResults %>%
      dplyr::select(STATION_ID, contains('_EXC')) %>% 
      group_by(STATION_ID) %>%
      pivot_longer(-STATION_ID, names_to = 'ParameterEXC', values_to = 'Exceedance') %>%
      mutate(individualColor = case_when(Exceedance > 1 ~ 'red',
                                         Exceedance == 1 ~ 'yellow',
                                         Exceedance == 0 ~ 'green',
                                         is.na(Exceedance) ~ 'gray'),
             # create a variable that ranks statuses to easily combine into single "score"
             individualScore = case_when(individualColor == 'red' ~ 1,
                                         individualColor == 'yellow' ~ 2,
                                         individualColor == 'green' ~ 3,
                                         individualColor == 'gray' ~ 4))
    # Gives one "rank" per station
    overall <- stationTableResults1 %>%
      group_by(STATION_ID, individualColor, individualScore) %>%
      dplyr::summarise(`n Parameters of lowest status` = n()) 
    # join number of ranks causing color info
    overall2 <- overall %>%
      group_by(STATION_ID) %>%
      summarise(stationOverallScore = min(individualScore)) %>%
      left_join(overall, by = c('STATION_ID','stationOverallScore' = 'individualScore')) %>%
      dplyr::select(-individualColor)
    
    return(left_join(stationTableResults1, overall2, by = 'STATION_ID') %>%
             mutate(stationColor = case_when(stationOverallScore == 1 ~ 'red',
                                             stationOverallScore == 2 ~ 'yellow',
                                             stationOverallScore == 3 ~ 'green',
                                             stationOverallScore == 4 ~ 'gray'),
                    stationOverallScore = as.factor(stationOverallScore), 
                    `Overall Station Result` = case_when(stationColor == 'red' ~ 'Station contains at least one parameter with 2 or more exceedances',
                                                         stationColor == 'yellow' ~ 'Station contains at least one parameter with one exceedance',
                                                         stationColor == 'green' ~ 'Station contains no exceedances',
                                                         stationColor == 'gray' ~ 'Station unassessed')) %>%
             left_join(parameterEXCcrosswalk, by = 'ParameterEXC' ) %>%
             left_join(dplyr::select(stationTableResults, STATION_ID, LATITUDE, LONGITUDE), by = 'STATION_ID') %>%
             ungroup() %>%
             st_as_sf(coords = c("LONGITUDE", "LATITUDE"), 
                      remove = T, # remove these lat/lon cols from df
                      crs = 4326) ) # add projection, needs to be geographic for now bc entering lat/lng
    
    # Gives ranked scale of n statuses
    #  overall <- vahu6StationSummary %>%
    #    group_by(STATION_ID, individualColor, individualScore) %>%
    #    dplyr::summarise(n = n()) %>%
    #    left_join(dplyr::select(stationTable, STATION_ID, LATITUDE, LONGITUDE), by = 'STATION_ID') %>%
    #    ungroup() %>%
    #    st_as_sf(coords = c("LONGITUDE", "LATITUDE"), 
    #             remove = T, # remove these lat/lon cols from df
    #             crs = 4269)  # add projection, needs to be geographic for now bc entering lat/lng
    #  
    #return(overall)
  } else {
    return(tibble(STATION_ID = NA, ParameterEXC = NA, Status = NA, individualColor = NA, 
                  individualScore = NA, stationOverallScore = NA, stationColor = NA, Parameter = NA)) }
  #return(tibble(STATION_ID = NA, individualColor = NA, individualScore = NA, n = NA)) }
}

assessmentSummary <- stationSummary(stationTableResults = left_join(assessmentResults$stationTableResults,
                                 dplyr::select(stationGIS_View, STATION_ID = Station_Id, LATITUDE = Latitude, LONGITUDE = Longitude),
                                 by = 'STATION_ID'),
                                 parameterEXCcrosswalk)

# Station Status Map Function
indStatusMap <- function(parameter, status){
  pal <- colorFactor(
    palette = c('red', 'yellow','green', 'gray'),
    domain = c(1, 2, 3, 4))
  
  if(parameter == 'Overall Status'){
    CreateWebMap(maps = c("Topo","Imagery","Hydrography"), collapsed = TRUE) %>%
      addCircleMarkers(data = status, color='black', fillColor=~pal(status$stationOverallScore), radius = 6,
                       fillOpacity = 0.5,opacity=1,weight = 2,stroke=T,group="Overall Station Status Summary",
                       label = ~STATION_ID, layerId = ~STATION_ID,
                       popup = leafpop::popupTable(status, zcol=c( "STATION_ID", "Overall Station Result", "n Parameters of lowest status"))  ) %>% 
      addLegend('topright', colors = c('red', 'yellow','green', 'gray'),
                labels = c('Station contains at least <br>one parameter with 2 or more exceedances',
                           'Station contains at least <br>one parameter with one exceedance',
                           'Station contains no exceedances',
                           'Station unassessed'), title = 'Legend') %>%
      addLayersControl(baseGroups=c("Topo","Imagery","Hydrography"),
                       overlayGroups = c('Overall Station Status Summary'),
                       options=layersControlOptions(collapsed=T),
                       position='topleft') 
  } else {
    indParameter <- filter(status, Parameter %in% parameter)
    
    CreateWebMap(maps = c("Topo","Imagery","Hydrography"), collapsed = TRUE) %>%
      addCircleMarkers(data = indParameter , color='black', fillColor=~pal(indParameter$individualScore), radius = 6,
                       fillOpacity = 0.5,opacity=1,weight = 2,stroke=T,group=paste(parameter, "Station Summary"),
                       label = ~STATION_ID, layerId = ~STATION_ID,
                       popup = leafpop::popupTable(indParameter , zcol=c( "STATION_ID", "Parameter", "Exceedance"))  ) %>% 
      addLegend('topright', colors = c('red', 'yellow','green', 'gray'),
                labels = c('Station has 2 or more exceedances',
                           'Station has one exceedance',
                           'Station has no exceedances', 
                           'Station unassessed'), title = 'Legend') %>%
      addLayersControl(baseGroups=c("Topo","Imagery","Hydrography"),
                       overlayGroups = c(paste(parameter, "Station Summary")),
                       options=layersControlOptions(collapsed=T),
                       position='topleft')  }
}

indStatusMap('Overall Status',assessmentSummary)
indStatusMap('pH',assessmentSummary)

```





















```{r test case BRRO ambient}
yrs <-  pool %>% tbl(in_schema("wqm", "Wqm_Yearly_Run_Sch_View")) %>%
  
  # filter by run year
  filter(Yrs_Year %in% c('2021')) %>% 
  
  ## filter by station selection
  #filter(Yrs_Sta_Id %in% !! station ) %>% 
  as_tibble() #%>%



mrs <- pool %>% tbl(in_schema("wqm", "Wqm_Monthly_Run_Sch_View")) %>%
  filter(Mrs_Id %in% !! filter(stationFieldData, Fdt_Sta_Id == "2-JKS018.68")$Fdt_Id) %>% 
  as_tibble()
  
stationFieldData$Fdt_Id 
```

```{r}
allStationsSince <- pool %>% tbl(in_schema("wqm", "Wqm_Field_Data_View")) %>%
  filter(between(as.Date(Fdt_Date_Time), !! dateRange[1], !! dateRange[2]) ) %>%
  filter(! Ssc_Description %in% "INVALID DATA SET QUALITY ASSURANCE FAILURE") %>% 
  dplyr::select(Fdt_Id, Fdt_Sta_Id) %>% 
  as_tibble() 

stationRegion <- pool %>% tbl(in_schema("wqm", "Wqm_Stations_View")) %>%
  filter(Sta_Id %in% !! allStationsSince$Fdt_Sta_Id) %>% 
  filter(Sta_Rec_Code %in% c("Blue Ridge")) %>% 
  as_tibble()
```
























```{r}
As_Station_Types=pool %>%
tbl(dbplyr::in_schema("wqa","Wqa_Station_Types_View")) %>%
filter(STL_TYPE_CODE %in% "C2") %>%
dplyr::select(STX_STATION_DETAIL_ID,STL_TYPE_DESCRIPTION)%>%
as_tibble()

jks <- pool %>%
tbl(dbplyr::in_schema("wqa","Wqa_Station_Types_View")) %>%
  filter(STX_STATION_DETAIL_ID == '57086') %>% 
  dplyr::select(STX_STATION_DETAIL_ID,STL_TYPE_DESCRIPTION)%>%
  as_tibble()

As_Sta_Details=pool %>%
tbl(dbplyr::in_schema("wqa","Wqa_Station_Details_View")) %>%
filter(WXA_STATION_DETAIL_ID %in% !!As_Station_Types$STX_STATION_DETAIL_ID) %>%
#filter(WXA_STATION_DETAIL_ID %in% !!jks$STX_STATION_DETAIL_ID) %>%
  as_tibble()

jks2<- pool %>%
tbl(dbplyr::in_schema("wqa","Wqa_Station_Details_View")) %>%
filter(WXA_STATION_DETAIL_ID %in% !!jks$STX_STATION_DETAIL_ID) %>%
  as_tibble()

stations <- pool %>%
tbl(dbplyr::in_schema("wqa","Wqa_Station_Details_View")) %>%
  filter(WSD_STATION_ID == '2-JKS023.61') %>% 
  #distinct(WSD_STATION_ID) %>% 
  as_tibble()
```
