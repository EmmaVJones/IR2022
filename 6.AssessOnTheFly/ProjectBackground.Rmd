---
title: "Background"
author: "Emma Jones"
date: "7/12/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)


httr::set_config(httr::config(ssl_verifypeer = FALSE, ssl_verifyhost = FALSE))

library(tidyverse)
library(config)
library(sf)
library(plotly)
library(lubridate)
library(pool)
library(pins)
library(sqldf)
library(dbplyr)


# Server connection things
conn <- config::get("connectionSettings") # get configuration settings


board_register_rsconnect(key = conn$CONNECT_API_KEY,  #Sys.getenv("CONNECT_API_KEY"),
                         server = conn$CONNECT_SERVER)#Sys.getenv("CONNECT_SERVER"))

## For testing: connect to ODS production
pool <- dbPool(
 drv = odbc::odbc(),
 Driver = "ODBC Driver 11 for SQL Server",#"SQL Server Native Client 11.0",
 Server= "DEQ-SQLODS-PROD,50000",
 dbname = "ODS",
 trusted_connection = "yes"
)
```

This project holds scripts to build an on the fly automated assessment program. There will be an application interface where users can select stations and data windows to run and explore interactively and another reporting method that will simplify results into a report emailed to users based on requested station and window info.

The process first queries "conventionals" data from CEDS ODS, then it runs the available automated assessment methods against the station data and summarizes it in a "stations table" format. Other functions can break this down into useable information for various user groups.

Below outlines the method that will be migrated into an application format.

```{r automated assessment needs}
# bring in methods and data for automated assessment to work
source('methods/conventionalsFunction.R')
source('methods/updatedBacteriaCriteria.R')
source('methods/3.automatedAssessment_global.R')
source('methods/automatedAssessmentFunctions.R')
lakeNutStandards <- read_csv('data/9VAC25-260-187lakeNutrientStandards.csv')

```



1) Identify station(s) and temporal window.

```{r identify stations}

station <- pin_get("conventionals2022IRfinalWithSecchi", board = "rsconnect") %>% 
  filter(STA_REC_CODE == 'BRRO') %>% 
  filter(str_detect(FDT_STA_ID, '2-JKS') ) %>% 
  distinct(FDT_STA_ID) %>% 
  pull()

dateRange <- c(as.Date('2020-01-01'), as.Date(Sys.Date()))

```



2) Pull corresponding field and analyte data. If too many records requested, run multiple queries.


```{r pull data}
stationFieldData <- pool %>% tbl(in_schema("wqm", "Wqm_Field_Data_View")) %>%
  filter(Fdt_Sta_Id %in% !! station &
           between(as.Date(Fdt_Date_Time), !! dateRange[1], !! dateRange[2]) ) %>%
    filter(! Ssc_Description %in% "INVALID DATA SET QUALITY ASSURANCE FAILURE") %>% 
  as_tibble()

stationAnalyteData <- pool %>% tbl(in_schema("wqm", "Wqm_Analytes_View")) %>%
  filter(Ana_Sam_Fdt_Id %in% !! stationFieldData$Fdt_Id &
           #between(as.Date(Ana_Received_Date), !! dateRange[1], !! dateRange[2]) & # x >= left & x <= right
           Pg_Parm_Name != "STORET STORAGE TRANSACTION DATE YR/MO/DAY") %>%
  as_tibble() %>%
  left_join(dplyr::select(stationFieldData, Fdt_Id, Fdt_Sta_Id, Fdt_Date_Time), by = c("Ana_Sam_Fdt_Id" = "Fdt_Id"))
stationInfo <- pool %>% tbl(in_schema("wqm",  "Wqm_Stations_View")) %>%
  filter(Sta_Id %in% !! toupper(station)) %>%
  as_tibble()
stationGIS_View <-  pool %>% tbl(in_schema("wqm",  "Wqm_Sta_GIS_View")) %>%
  filter(Station_Id %in% !! toupper(station)) %>%
  as_tibble()
conventionals <- conventionalsSummary(conventionals= pin_get("conventionals2022IRfinalWithSecchi", board = "rsconnect")[0,],
                           stationFieldDataUserFilter= stationFieldData, stationAnalyteDataUserFilter = stationAnalyteData,
                           stationInfo,
                           stationGIS_View,
                           dropCodes = c('QF'))%>% 
  arrange(FDT_STA_ID, FDT_DATE_TIME, FDT_DEPTH) #%>% 
```

3) Organize metadata. Keep this separate so users can see information and adjust stations to be lake/lacustrine as needed.

```{r metadata}
WQSlookup <- pin_get("WQSlookup-withStandards",  board = "rsconnect")
WQMstationSpatial <- pin_get("WQM-Stations-Spatial", board = "rsconnect")
VSCIresults <- pin_get("VSCIresults", board = "rsconnect") %>%
  filter( between(`Collection Date`, dateRange[1], dateRange[2]) )

stationTable <- left_join(tibble(STATION_ID = station),
                          WQSlookup, by = c('STATION_ID'='StationID')) %>%
  mutate(CLASS_BASIN = paste(CLASS,substr(BASIN, 1,1), sep="_")) %>%
  mutate(CLASS_BASIN = ifelse(CLASS_BASIN == 'II_7', "II_7", as.character(CLASS))) %>%
  # Fix for Class II Tidal Waters in Chesapeake (bc complicated DO/temp/etc standard)
  left_join(WQSvalues, by = 'CLASS_BASIN') %>%
  dplyr::select(-c(CLASS.y,CLASS_BASIN)) %>%
  rename('CLASS' = 'CLASS.x') %>%
  left_join(WQMstationSpatial %>% distinct(StationID, .keep_all = TRUE), by = c('STATION_ID' = 'StationID')) %>%
  # last cycle had code to fix Class II Tidal Waters in Chesapeake (bc complicated DO/temp/etc standard) but not sure if necessary
  lakeNameStandardization() %>% # standardize lake names
  
   
  # extra special step
  mutate(Lake_Name = case_when(STATION_ID %in% c('2-TRH000.40') ~ 'Thrashers Creek Reservoir',
                               STATION_ID %in% c('2-LSL000.16') ~ 'Lone Star Lake F (Crystal Lake)',
                               STATION_ID %in% c('2-LSL000.04') ~ 'Lone Star Lake G (Crane Lake)',
                               STATION_ID %in% c('2-LSL000.20') ~ 'Lone Star Lake I (Butler Lake)',
                               STATION_ID %in% c('2-NWB002.93','2-NWB004.67', '2-NWB006.06') ~ 'Western Branch Reservoir',
                               STATION_ID %in% c('2-LDJ000.60') ~ 'Lake Nottoway (Lee Lake)',
                               TRUE ~ as.character(Lake_Name))) %>%
  left_join(lakeNutStandards %>% 
              mutate(Lakes_187B = 'y'),  # special step to make sure the WQS designation for 187 are correct even when not
            by = c('Lake_Name')) %>%
  # lake drummond special standards
  mutate(Lakes_187B = ifelse(is.na(Lakes_187B.y ), Lakes_187B.x, Lakes_187B.y), 
    `Chlorophyll a (ug/L)` = case_when(Lake_Name %in% c('Lake Drummond') ~ 35,
                                            TRUE ~ as.numeric(`Chlorophyll a (ug/L)`)),
         `Total Phosphorus (ug/L)` = case_when(Lake_Name %in% c('Lake Drummond') ~ 40,
                                               TRUE ~ as.numeric(`Total Phosphorus (ug/L)`))) %>% 
  dplyr::select(STATION_ID:StreamType, Lakes_187B, `Description Of Waters`:`Total Phosphorus (ug/L)`)





# Need to find way to identify lake stations
# # Traditional assessment method won't work
# lakeStations <- filter(stationTable, str_detect(ID305B_1, 'L_') | str_detect(ID305B_2, 'L_') | str_detect(ID305B_3, 'L_') | str_detect(ID305B_4, 'L_') |
#                            str_detect(ID305B_5, 'L_') | str_detect(ID305B_6, 'L_') | str_detect(ID305B_7, 'L_') | str_detect(ID305B_8, 'L_') |
#                            str_detect(ID305B_9, 'L_') | str_detect(ID305B_10, 'L_') )

# using just a vector of names for now, ideally will be sourced by providing and DT::datatable with checkbox field and using the checkbox information to
# identify whether a station is a lake station or not
lakeStations <- filter(stationTable, STATION_ID %in% c('2-JKS044.60', '2-JKS046.40', '2-JKS048.90', '2-JKS053.48'))
lacustrineDesignation <- filter(stationTable, STATION_ID %in% c('2-JKS044.60', '2-JKS046.40'))
```



3) Assess data

```{r automated assessment function}
automatedAssessmentFunction <- function(stationTable, conventionals){
  stationTableResults <- tibble()
# save ammonia results (based on default assessment information) for use in app to speed rendering
ammoniaAnalysis <- tibble()

for(i in 1:nrow(stationTable)){
#i = 1
  print(paste('Assessing station', i, 'of', nrow(stationTable), sep=' '))

  # pull one station data
  stationData <- filter(conventionals, FDT_STA_ID %in% stationTable$STATION_ID[i]) %>%
    left_join(stationTable, by = c('FDT_STA_ID' = 'STATION_ID')) %>%
    pHSpecialStandardsCorrection() %>% # correct pH to special standards where necessary
    # special lake steps
    {if(stationTable$STATION_ID[i] %in% lakeStations$STATION_ID)
      suppressWarnings(suppressMessages(
        mutate(., lakeStation = TRUE) %>%
        thermoclineDepth())) # adds thermocline information and SampleDate
      else mutate(., lakeStation = FALSE) } %>% 
    # manually add lacustrine designation per user input
    mutate(LACUSTRINE = ifelse(stationTable$STATION_ID[i] %in% lacustrineDesignation$STATION_ID, TRUE, FALSE))
   

  # If data exists for station, run it
  if(nrow(stationData) > 0){
    # Date last sampled
    dateLastSampled <- as.character(max(stationData$FDT_DATE_TIME)) } else {dateLastSampled <- 'No data in conventionals data pull'}

   
  # Ammonia special section
  ammoniaAnalysisStation <- freshwaterNH3limit(stationData, trout = ifelse(unique(stationData$CLASS) %in% c('V','VI'), TRUE, FALSE),
                                        mussels = TRUE, earlyLife = TRUE) 
  # https://law.lis.virginia.gov/admincode/title9/agency25/chapter260/section155/ states the assumption is that
  #  waters are to be assessed with the assumption that mussels and early life stages of fish should be present
  # trout presence is determined by WQS class, this can be changed in the app but is forced to be what the station
  # is attributed to in the automated assessment scripts
  
     
  if(nrow(stationData) > 0){
    # PWS stuff
    if(is.na(unique(stationData$PWS))  ){
      PWSconcat <- tibble(#STATION_ID = unique(stationData$FDT_STA_ID),
                          PWS= NA)
   } else {
     PWSconcat <- cbind(#tibble(STATION_ID = unique(stationData$FDT_STA_ID)),
                         assessPWS(stationData, NITRATE_mg_L, LEVEL_NITRATE, 10, 'PWS_Nitrate'),
                         assessPWS(stationData, CHLORIDE_mg_L, LEVEL_CHLORIDE, 250, 'PWS_Chloride'),
                         assessPWS(stationData, SULFATE_TOTAL_mg_L, LEVEL_SULFATE_TOTAL, 250, 'PWS_Total_Sulfate')) %>%
       dplyr::select(-ends_with('exceedanceRate')) }
    
  # chloride assessment if data exists
    if(nrow(filter(stationData, !is.na(CHLORIDE_mg_L))) > 0){
      chlorideFreshwater <- chlorideFreshwaterSummary(suppressMessages(chlorideFreshwaterAnalysis(stationData)))
    } else {chlorideFreshwater <- tibble(CHL_EXC = NA, CHL_STAT= NA)}
    
  # Nutrients based on station type
    # Nutrient: TP (lakes have real standards; riverine no longer uses 0.2 mg/L as an observed effect for Aquatic life use but will use it as flag for this)
    if(unique(stationData$lakeStation) == TRUE){
      TP <- TP_Assessment(stationData) 
      } else {
        TP <- countNutrients(stationData, PHOSPHORUS_mg_L, LEVEL_PHOSPHORUS, 0.2) %>% quickStats('NUT_TP') %>%   # using 0.2mg/L as flag for this "assessment"
          mutate(NUT_TP_STAT = ifelse(NUT_TP_STAT != "S", "Review", NA)) } # flag OE but don't show a real assessment decision
    
      # Nutrients: Chl a (lakes)
    if(unique(stationData$lakeStation) == TRUE){
      chla <- chlA_Assessment(stationData)
        #tibble(NUT_CHLA_EXC = NA, NUT_CHLA_SAMP = NA, NUT_CHLA_STAT = NA) # placeholder for now
      } else {
        chla <- countNutrients(stationData, CHLOROPHYLL_A_ug_L, LEVEL_CHLOROPHYLL_A, NA) %>% quickStats('NUT_CHLA') %>%
          mutate(NUT_CHLA_STAT = NA) } # don't show a real assessment decision
    
    # run DO daily average status for riverine and tuck results into comments later
    if(unique(stationData$lakeStation) == TRUE){
      DO_Daily_Avg_STAT <- ''
    } else {
      DO_Daily_Avg_STAT <- paste0('DO_Daily_Avg_STAT: ', 
                                       DO_Assessment_DailyAvg(stationData) %>% 
                                         quickStats('DO_Daily_Avg') %>% 
                                         dplyr::select(DO_Daily_Avg_STAT) %>% pull())}
  
  
  results <- cbind(
      dplyr::select(stationData, STATION_ID = FDT_STA_ID, Sta_Desc:BASIN_CODE, Basin_Code= Basin_Code.y, CountyCityName:Location) %>% 
        distinct(STATION_ID, .keep_all = T),
      tempExceedances(stationData) %>% quickStats('TEMP'),
      DOExceedances_Min(stationData) %>% quickStats('DO'), 
      # this will be removed for lake stations later since it does not apply
      pHExceedances(stationData) %>% quickStats('PH'),
      bacteriaAssessmentDecisionClass(stationData),
      ammoniaDecision(list(acute = freshwaterNH3Assessment(ammoniaAnalysisStation, 'acute'),
                           chronic = freshwaterNH3Assessment(ammoniaAnalysisStation, 'chronic'),
                           fourDay = freshwaterNH3Assessment(ammoniaAnalysisStation, 'four-day'))), 
    
      # Will add in metals assessment at a later date
      
      # PCB and fish info can only be incorporated when data migrated into CEDS
      
      # Benthics, just a flag that benthic data exists
      benthicAssessment(stationData, VSCIresults),
      
      # Nutrient Assessment done above by waterbody type
      TP,
      chla) %>%
    # COMMENTS
      mutate(COMMENTS = paste0(DO_Daily_Avg_STAT, 
                               ' | AMMONIA Comment: ', `Assessment Decision`,
            BACTERIA_COMMENTS = paste0(' | E.coli Comment: ', ECOLI_STATECOLI_VERBOSE,
                              ' | Enterococci Comment: ', ENTER_STATENTER_VERBOSE)) ) %>%
      dplyr::select(-ends_with(c('exceedanceRate', 'VERBOSE', 'Assessment Decision', 'StationID'))) %>%  # to match Bulk Upload template but helpful to keep visible til now for testing
      mutate(`Date Last Sampled` = dateLastSampled) 
  } else {# pull what you can from last cycle and flag as carry over
    results <- filter(stationTable, STATION_ID == stationTable$STATION_ID[i]) %>%
      dplyr::select(STATION_ID , Sta_Desc:BASIN_CODE, Basin_Code= Basin_Code.y, CountyCityName:Location) %>% 
        distinct(STATION_ID, .keep_all = T) %>% 
      mutate(COMMENTS = 'This station has no data in current window.') %>%
      mutate(`Date Last Sampled` = dateLastSampled)
  }
    stationTableResults <- bind_rows(stationTableResults, results) %>% 
      relocate(c( BACTERIADECISION, BACTERIASTATS), .after = `Date Last Sampled`)
  ammoniaAnalysis <- bind_rows(ammoniaAnalysis, tibble(StationID = unique(stationData$FDT_STA_ID), AmmoniaAnalysis = list(ammoniaAnalysisStation)))
}

return(list(stationTableResults = stationTableResults, ammoniaAnalysis = ammoniaAnalysis))
}

```


How to run assessment

```{r run assessment}
z <- automatedAssessmentFunction(stationTable, conventionals)
z$stationTableResults
z$ammoniaAnalysis

```



### Practice exercise to get stations sampled by a region from beginning of year.


Try going from people to data 

```{r BRRO since start of year practice}
dateRange <- c(as.Date('2021-01-01'), as.Date(Sys.Date()))
region <- c('SCRO', 'WCRO')# convert BRRO to codes manually until fixed
spgCodes <- NULL#c('IM', 'TM')

if(!is.null(region)){
  peeps <- pool %>% tbl(in_schema("wqm", "Wqm_Collector_Cds_Codes_Wqm_View")) %>% 
  filter(Unt_Rec_Code %in% !! region) %>% 
  as_tibble()
  } else {peeps <- tibble()}


stationFieldData <- pool %>% tbl(in_schema("wqm", "Wqm_Field_Data_View")) %>%
  filter(between(as.Date(Fdt_Date_Time), !! dateRange[1], !! dateRange[2]) ) %>% # date range always required
  # if Region (aka people) selected
  {if(nrow(peeps) > 0)
    filter(., Fdt_Collector_Id %in% !! peeps$Col_Id )
    else . } %>%
  # if Program Code selected
  {if(!is.null(spgCodes))
    filter(., Fdt_Spg_Code %in% !! spgCodes)
    else . } %>% 
  # always drop crap data
  filter(! Ssc_Description %in% "INVALID DATA SET QUALITY ASSURANCE FAILURE") %>% 
  as_tibble() 

# still need this for other functions
station <- unique(stationFieldData$Fdt_Sta_Id)
           
stationAnalyteData <- pool %>% tbl(in_schema("wqm", "Wqm_Analytes_View")) %>%
  filter(Ana_Sam_Fdt_Id %in% !! stationFieldData$Fdt_Id &
           #between(as.Date(Ana_Received_Date), !! dateRange[1], !! dateRange[2]) & # x >= left & x <= right
           Pg_Parm_Name != "STORET STORAGE TRANSACTION DATE YR/MO/DAY") %>%
  as_tibble() %>%
  left_join(dplyr::select(stationFieldData, Fdt_Id, Fdt_Sta_Id, Fdt_Date_Time), by = c("Ana_Sam_Fdt_Id" = "Fdt_Id"))

stationInfo <- pool %>% tbl(in_schema("wqm",  "Wqm_Stations_View")) %>%
  filter(Sta_Id %in% !! toupper(station)) %>%
  as_tibble()

stationGIS_View <-  pool %>% tbl(in_schema("wqm",  "Wqm_Sta_GIS_View")) %>%
  filter(Station_Id %in% !! toupper(station)) %>%
  as_tibble()

conventionals <- conventionalsSummary(conventionals= pin_get("conventionals2022IRfinalWithSecchi", board = "rsconnect")[0,],
                           stationFieldDataUserFilter= stationFieldData, stationAnalyteDataUserFilter = stationAnalyteData,
                           stationInfo,
                           stationGIS_View,
                           dropCodes = c('QF'))%>% 
  arrange(FDT_STA_ID, FDT_DATE_TIME, FDT_DEPTH) #%>% 

stationTable <- left_join(tibble(STATION_ID = station),
                          WQSlookup, by = c('STATION_ID'='StationID')) %>%
  mutate(CLASS_BASIN = paste(CLASS,substr(BASIN, 1,1), sep="_")) %>%
  mutate(CLASS_BASIN = ifelse(CLASS_BASIN == 'II_7', "II_7", as.character(CLASS))) %>%
  # Fix for Class II Tidal Waters in Chesapeake (bc complicated DO/temp/etc standard)
  left_join(WQSvalues, by = 'CLASS_BASIN') %>%
  dplyr::select(-c(CLASS.y,CLASS_BASIN)) %>%
  rename('CLASS' = 'CLASS.x') %>%
  left_join(WQMstationSpatial %>% distinct(StationID, .keep_all = TRUE), by = c('STATION_ID' = 'StationID')) %>%
  # last cycle had code to fix Class II Tidal Waters in Chesapeake (bc complicated DO/temp/etc standard) but not sure if necessary
  lakeNameStandardization() %>% # standardize lake names
  
   
  # extra special step
  mutate(Lake_Name = case_when(STATION_ID %in% c('2-TRH000.40') ~ 'Thrashers Creek Reservoir',
                               STATION_ID %in% c('2-LSL000.16') ~ 'Lone Star Lake F (Crystal Lake)',
                               STATION_ID %in% c('2-LSL000.04') ~ 'Lone Star Lake G (Crane Lake)',
                               STATION_ID %in% c('2-LSL000.20') ~ 'Lone Star Lake I (Butler Lake)',
                               STATION_ID %in% c('2-NWB002.93','2-NWB004.67', '2-NWB006.06') ~ 'Western Branch Reservoir',
                               STATION_ID %in% c('2-LDJ000.60') ~ 'Lake Nottoway (Lee Lake)',
                               TRUE ~ as.character(Lake_Name))) %>%
  left_join(lakeNutStandards %>% 
              mutate(Lakes_187B = 'y'),  # special step to make sure the WQS designation for 187 are correct even when not
            by = c('Lake_Name')) %>%
  # lake drummond special standards
  mutate(Lakes_187B = ifelse(is.na(Lakes_187B.y ), Lakes_187B.x, Lakes_187B.y), 
    `Chlorophyll a (ug/L)` = case_when(Lake_Name %in% c('Lake Drummond') ~ 35,
                                            TRUE ~ as.numeric(`Chlorophyll a (ug/L)`)),
         `Total Phosphorus (ug/L)` = case_when(Lake_Name %in% c('Lake Drummond') ~ 40,
                                               TRUE ~ as.numeric(`Total Phosphorus (ug/L)`))) %>% 
  dplyr::select(STATION_ID:StreamType, Lakes_187B, `Description Of Waters`:`Total Phosphorus (ug/L)`)

# using just a vector of names for now, ideally will be sourced by providing and DT::datatable with checkbox field and using the checkbox information to
# identify whether a station is a lake station or not
lakeStations <- filter(stationTable, STATION_ID %in% c('2-JKS044.60', '2-JKS046.40', '2-JKS048.90', '2-JKS053.48'))
lacustrineDesignation <- filter(stationTable, STATION_ID %in% c('2-JKS044.60', '2-JKS046.40'))

# run assessment
assessmentResults <- automatedAssessmentFunction(stationTable, conventionals)
View(assessmentResults$stationTableResults)
assessmentResults$ammoniaAnalysis


```




```{r}
As_Station_Types=pool %>%
tbl(dbplyr::in_schema("wqa","Wqa_Station_Types_View")) %>%
filter(STL_TYPE_CODE %in% "C2") %>%
dplyr::select(STX_STATION_DETAIL_ID,STL_TYPE_DESCRIPTION)%>%
as_tibble()

jks <- pool %>%
tbl(dbplyr::in_schema("wqa","Wqa_Station_Types_View")) %>%
  filter(STX_STATION_DETAIL_ID == '57086') %>% 
  dplyr::select(STX_STATION_DETAIL_ID,STL_TYPE_DESCRIPTION)%>%
  as_tibble()

As_Sta_Details=pool %>%
tbl(dbplyr::in_schema("wqa","Wqa_Station_Details_View")) %>%
filter(WXA_STATION_DETAIL_ID %in% !!As_Station_Types$STX_STATION_DETAIL_ID) %>%
#filter(WXA_STATION_DETAIL_ID %in% !!jks$STX_STATION_DETAIL_ID) %>%
  as_tibble()

jks2<- pool %>%
tbl(dbplyr::in_schema("wqa","Wqa_Station_Details_View")) %>%
filter(WXA_STATION_DETAIL_ID %in% !!jks$STX_STATION_DETAIL_ID) %>%
  as_tibble()

stations <- pool %>%
tbl(dbplyr::in_schema("wqa","Wqa_Station_Details_View")) %>%
  filter(WSD_STATION_ID == '2-JKS023.61') %>% 
  #distinct(WSD_STATION_ID) %>% 
  as_tibble()
```


```{r test case BRRO ambient}
yrs <-  pool %>% tbl(in_schema("wqm", "Wqm_Yearly_Run_Sch_View")) %>%
  
  # filter by run year
  filter(Yrs_Year %in% c('2021')) %>% 
  
  ## filter by station selection
  #filter(Yrs_Sta_Id %in% !! station ) %>% 
  as_tibble() #%>%



mrs <- pool %>% tbl(in_schema("wqm", "Wqm_Monthly_Run_Sch_View")) %>%
  filter(Mrs_Id %in% !! filter(stationFieldData, Fdt_Sta_Id == "2-JKS018.68")$Fdt_Id) %>% 
  as_tibble()
  
stationFieldData$Fdt_Id 
```

```{r}
allStationsSince <- pool %>% tbl(in_schema("wqm", "Wqm_Field_Data_View")) %>%
  filter(between(as.Date(Fdt_Date_Time), !! dateRange[1], !! dateRange[2]) ) %>%
  filter(! Ssc_Description %in% "INVALID DATA SET QUALITY ASSURANCE FAILURE") %>% 
  dplyr::select(Fdt_Id, Fdt_Sta_Id) %>% 
  as_tibble() 

stationRegion <- pool %>% tbl(in_schema("wqm", "Wqm_Stations_View")) %>%
  filter(Sta_Id %in% !! allStationsSince$Fdt_Sta_Id) %>% 
  filter(Sta_Rec_Code %in% c("Blue Ridge")) %>% 
  as_tibble()
```


