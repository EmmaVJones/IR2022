---
title: "Reworking Statewide Data Preprocessing"
author: "Emma Jones"
date: "2/18/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(sf)
library(readxl)
library(miniUI)
library(shiny)
library(leaflet)
library(mapview)

source('snappingFunctions/snapFunctions.R') # snapping functions
source('snappingFunctions/snapOrganizationFunctions_v2.R') # functions to do stuff with snapping functions

```


This project is a stream of consciousness walk through of how to rebuild the data preprocessing steps of the automated assessment methodology. 

## Data Organization

### Conventionals

This script was taken from the 2020 ambient network planning project. It combines the 2020IR conventionals data pull (2013-2018) with a special data pull for the ambient planning project that encompassed 2018-2019. 

```{r conventionals}
# Bring in 2020 IR data pull (2013-2018 data), will still need to bring in 2017&2018 sites
# Goal: filter out all BRRO sites to get a list of which sites were sampled each year
#  and frequency if possible
conventionals <- read_csv('C:/HardDriveBackup/R/GitHub/Rivers-StreamsAssessment/R&S_app_v4/data/final2020data/CEDSWQM_2020_IR_DATA-CONVENTIONALS_20190305.csv') 
conventionals$FDT_DATE_TIME2 <- as.POSIXct(conventionals$FDT_DATE_TIME, format="%m/%d/%Y %H:%M")

#summary(conventionals$FDT_DATE_TIME2)

# Now add recent data (2018-Nov 2019- the day Roger made the data pull)
# already limited to BRRO (SCRO and WCRO)
conventionals2 <- read_excel('C:/HardDriveBackup/R/GitHub/AmbientNetworkPlanning/for2020/data/CEDSWQM_CONVENTIONALS_2018+.xlsx')
conventionals2$FDT_DATE_TIME2 <- as.POSIXct(conventionals2$FDT_DATE_TIME, format="%m/%d/%Y %H:%M")
conventionals2$FDT_DATE_TIME <- as.character(conventionals2$FDT_DATE_TIME) # for smashing with original conventionals
summary(conventionals2$FDT_DATE_TIME2)

# filter data to just 2019 to not duplicate data from 2018
conventionals2019 <- filter(conventionals2, FDT_DATE_TIME2 > '2018-12-31 23:59:00')
summary(conventionals2019$FDT_DATE_TIME2)
# cool.
#glimpse(conventionals2019)

# what is in conventionals that isn't in conventionals2019??
names(conventionals)[!names(conventionals) %in% names(conventionals2019)]

conventionalsAll <- bind_rows(conventionals,conventionals2019) %>%
  # get groundwater sites out of here
  filter(FDT_SPG_CODE != 'GW')

conventionals <- filter(conventionalsAll, !is.na(Latitude)|!is.na(Longitude)) # remove sites without coordinates

rm(conventionalsAll);rm(conventionals2);rm(conventionals2019)
```

So we will use this as a pretend dataset to simuate what one could expect from 2022 IR, e.g. some attributed (AU and WQS) stations as well as some new stuff. This exercise will be conducted for BRRO, but the methods will apply statewide.

Before we can do any spatial subsetting work, we need to make a dataset of all UNIQUE StationID's for the regional offices to work with. 

```{r conventionalsDistinct, echo=FALSE}
conventionals_D <- distinct(conventionals, FDT_STA_ID, .keep_all = T) %>%
  select(FDT_STA_ID:FDT_SPG_CODE, Latitude:STA_CBP_NAME) # drop data to avoid any confusion

rm(conventionals) # remove full conventionals dataset to save memory

#View(conventionals_D) # to look at dataset use this command
```



### Single Input Dataset

By combining the conventionals, citmon, and non agency data, we can have a single starting point. We are also going to drop everything but station name and lat/lng as our joining information to avoid duplicating too much data.

```{r one input dataset}
irData <-# bind_rows(cit, nonA) %>%
  #bind_rows(conventionals_D) %>%
  #dplyr::select(Group_Station_ID, FDT_STA_ID, Latitude, Longitude)
  mutate(conventionals_D, UID = row_number()) %>% # create a UID for QA purposes
  dplyr::select(UID, FDT_STA_ID, Latitude, Longitude)


```


### Last Cycle Stations Table 2.0

Joining last cycle's station table 2.0 (station table with AU and WQS information) will save a lot of effort this year.

Since we are using BRRO as the testing region, we will bring in their final stations table 2.0 as a starting point. As this is scaled statewide we will smash all regions stations table 2.0 together as a starting point.

Since leftover 2018 IR results are still tucked in there we want to drop those columns to not confuse anyone.

```{r station table 2.0} 
RegionalResultsCombined <- read_csv('C:/HardDriveBackup/R/GitHub/Rivers-StreamsAssessment/R&S_app_v4/processedStationData/RegionalResultsRiverine_BRROCitMonNonAgencyFINAL.csv') %>%
  bind_rows(read_csv('C:/HardDriveBackup/R/GitHub/LakesAssessment2020/app2020/LakeAssessmentApp_citmon/processedStationData/lakeStations2020_BRRO_citmonNonAgency.csv')) %>%
  dplyr::select(FDT_STA_ID:VAHU6, Point.Unique.Identifier:Assess_TYPE) %>%
  mutate(Huc6_Huc_8 = paste0(0, Huc6_Huc_8))
```


## Initial Join: Free Metadata

So by using a simple join to existing information that has been QAed by a human, we can limit the amount of computational work and additional human QA.

```{r joinTime}
irData_BRRO <- filter(irData, str_detect(FDT_STA_ID,'9-'))

irData_join <- dplyr::left_join(irData_BRRO, RegionalResultsCombined,
                  by = c('FDT_STA_ID', 'Latitude', 'Longitude')) %>%
  st_as_sf(coords = c("Longitude", "Latitude"),  # make spatial layer using these columns
           remove = F, # don't remove these lat/lon cols from df
           crs = 4326) # add coordinate reference system, needs to be geographic for now bc entering lat/lng, 
```


## Just missing data from here

Now we will only deal with stations that did not join to the previous stations table by name, lat, and long. These stations are missing all CEDS attribute information, so that will be repopulated from the CEDS pull or James' notes.

```{r irData missing}
irData_missing <- filter(irData_join, is.na(ID305B_1)) # these had no previous records

irData_final <- filter(irData_join, !is.na(ID305B_1))

# make sure we didn't lose anyone
nrow(irData_final) + nrow(irData_missing) == nrow(irData_join)
```

Join back previously populated station information.


```{r repopulate station information}
irData_missing <- #filter(cit, FDT_STA_ID %in% irData_missing$FDT_STA_ID) %>%
   #bind_rows(filter(nonA, FDT_STA_ID %in% irData_missing$FDT_STA_ID)) %>%
  #bind_rows(filter(conventionals_D, FDT_STA_ID %in% irData_missing$FDT_STA_ID))
  left_join(dplyr::select(irData_missing, UID:Longitude), 
            conventionals_D, by = c('FDT_STA_ID','Latitude', 'Longitude')) %>%
  dplyr::select(-geometry,geometry)

rm(conventionals_D)#; rm(cit); rm(nonA)

```



## Assessment Units

### Statewide Assessment Layer

We are going to first associate a river basin with each station to make processing easier.

First, read in the statewide assessment layer.

```{r Statewide Assessment Layer, echo=FALSE, message=FALSE, warning=FALSE, results = 'hide'}
assessmentLayer <- st_read('GIS/AssessmentRegions_VA84_basins.shp') %>%
  st_transform( st_crs(4326)) # transform to WQS84 for spatial intersection
``` 

Then join to irData_join if the stations did not connect to the previous stations table.

```{r basin connection}
irData_missing <- st_join(irData_missing, assessmentLayer, join = st_intersects) %>%
    #filter(!is.na(VAHU6)) %>% # only keep rows that joined
  mutate(sameVAHU6 = ifelse(VAHU6 == Huc6_Vahu6, T, F)) %>% # double check the VAHU6 watershed matches from CEDS to actual spatial layer
  dplyr::select(UID:STA_CBP_NAME, Basin, VAHU6, sameVAHU6) %>% # just get columns we are interested in, compare here if desired
  dplyr::select(UID:STA_CBP_NAME,Basin)
```


#### Last Cycle Estuary Assessment Layer

First thing we want to do now is filter out any sites that fall into the estuary AUs. These stations will have to be assessed through other means. **Note: you can download the table below for reference by running the commented script. These are all the stations within the selected assessment region that fall into an estuary AU based on the 2018 AU layers.**

The below chunk:
1) Reads in the 2018 estuary AU layer
2) Finds any sites that fall into an estuary AU polygon
  - see optional code to write those sites to a separate csv for future reference
3) Removes any estuarine sites from the data frame of unique conventionals sites that need AU and WQS information.

```{r Filter Out Estuary Statewide, echo=F, results = 'hide'}
estuaryAUs <- st_read('C:/HardDriveBackup/GIS/Assessment/va_2018_aus_estuarine.shp') %>%
   st_transform( st_crs(4326)) # transform to WQS84 for spatial intersection


irData_missing_Estuary <- st_join(irData_missing, estuaryAUs, join = st_intersects) %>%
      filter(!is.na(OBJECTID))

#write.csv(irData_missing_Estuary, 'irData_missing_Estuary.csv', row.names = F) 
# you can change the file location like so:
#write.csv(irData_missing_Estuary, 'C:/Assessment/2020/dataProcessing/irData_missing_Estuary.csv', row.names = F) 

irData_missing_NoEstuary <- filter(irData_missing, !(FDT_STA_ID %in% irData_missing_Estuary$FDT_STA_ID))
rm(estuaryAUs) # remove from memory
```

Below is the table of sites that are estuarine.

```{r estuarySitesTable, echo=F}
DT::datatable(irData_missing_Estuary, escape=F, rownames = F, options = list(scrollX = TRUE))
#View(irData_missing_Estuary)
```

```{r inlineText2,echo=F}
cat(paste('Removing the above sites, the unique sites have gone from ', nrow(irData_missing),' to ',nrow(irData_missing_NoEstuary),'. The next step is to find the lake/reservoir stations and link them to the appropriate AU and WQS for analyses in the Lake Assessment App.', sep=''))
```


#### Last Cycle Reservoir Assessment Layer 

```{r inlineText3, echo=F}
cat(paste('Working with the ', nrow(irData_missing_NoEstuary),' sites that did not connect to a STATION_ID/FDT_STA_ID from last cycle, we will first do a spatial join to the reservoir/lakes AUs to see if any sites are lake sites before using the more computationally intensive stream snapping process.', sep=''))
```

```{r reservoirStatewide, echo=F, results = 'hide'}
reservoirAUs <- st_read('C:/HardDriveBackup/GIS/Assessment/va_2018_aus_reservoir.shp') %>%
   st_transform( st_crs(4326)) # transform to WQS84 for spatial intersection

# find lake AUs, if Any
irData_missing_Lakes <- st_join(irData_missing_NoEstuary, reservoirAUs, join = st_intersects) %>%
  filter(!is.na(OBJECTID)) %>% # only keep rows where join successful
  mutate(ID305B_1 = ID305B) %>% # rename ID305B_1 field by the joined reservoir ID305B
  dplyr::select(names(irData_missing_NoEstuary), ID305B_1)

# add lake AU sites to conventionals_D_Region_AU
#conventionals_D_Region_AU <- rbind(conventionals_D_Region_AU, conventionals_D_Region_Lakes)

# and remove lake AU sites from the sites without AUs
irData_missing_NoAU <- filter(irData_missing, !(FDT_STA_ID %in% irData_missing_Lakes$FDT_STA_ID)) %>%
  mutate(ID305B_1 = NA) # need column for snapping tools
rm(reservoirAUs) # remove from memory
```

#### Last Cycle Riverine Assessment Layer

```{r inlineText4, echo=F}
cat(paste('The last step for AU organization after we have removed any estuary sites and dealt with all sites that are in lake AUs is to automatically snap the', nrow(irData_missing_NoAU), 'remaining sites to riverine AUs.',sep=' '))
```

A custom, automated snapping tool is run below. The function buffers each site using the user defined sequence (bufferDistances). You do not need to change the bufferDistances variable, but commented out code demonstrates how you can if you want to increase/decrease the maximum buffer distance or breaks. 

The function first buffers the site the lowest input buffer distance, then it intersects that buffer with the input assessment unit layer. If no segments are retrieved in the given buffer distance, then the next highest buffer is tried and so on until the maximum buffer distance is tested. If one or more segments are captured in a given buffer, then the buffering process stops, returns the snapped stream segment information, and moves to the next site. If more that one site is captured in a single buffer, then those segments will be provided to the user to choose from in an interactive gadget to keep only one AU per site after all buffering has finished. If the maximum buffer distance does not yield any stream segments, then those sites will be returned to the user for special QA and manual table completion after the report is finished. It is advised that you investigate the site further and potentially correct site coordinates in CEDS to prevent future problems. 

**Note: All stations you want to use in the assessment applications require AU information. If no AU information is provided to the app with a StationID, then the station will not appear in the app and could potentially be lost until further QA.**

The next chunk will bring in the riverine AU layer from the previous cycle, complete the snapping tool, ask for user input in the form of a gadget in the viewer panel if sites snap to multiple segments in a single buffer, and then organize results. You can see the progress and results of the tool below the chunk.

```{r riverineAUsnap, echo=FALSE}
# use a custom snapping tool to snap and organize 
riverineAUs <-  st_read('C:/HardDriveBackup/GIS/Assessment/va_2018_aus_riverine.shp') %>%
  st_transform(crs = 102003)  # transform to Albers for spatial intersection

#Regional_Sites_AU <- snapAndOrganizeAU_noUI(filter(irData_missing_NoAU, FDT_STA_ID %in% #c('9-BFK003.33','9-LHC001.92'), riverineAUs,  bufferDistances = seq(10,50,by=10))

Regional_Sites_AU <- snapAndOrganizeAU(irData_missing_NoAU, riverineAUs,  bufferDistances = seq(10,50,by=10))

rm(riverineAUs) #clean up workspace
```

We can now filter the results to figure out sites that worked and sites that still need special attention. 

Below are the sites that still need work.
```{r conventionals_D_Region_NoAU still, echo=F}
conventionals_D_Region_NoAU <- filter(Regional_Sites_AU, is.na(ID305B_1))
DT::datatable(conventionals_D_Region_NoAU, escape=F, rownames = F, options = list(scrollX = TRUE))
#View(conventionals_D_Region_NoAU)
```

These sites will still be snapped to WQS, but they still need AU information before they can be run through the assessment applications. A reminder at the end of the report will encourage users to complete that step.


Below are the sites that worked. These can be combined with our conventionals_D_Region_AU layer now. 
```{r Regional_Sites_AUgood, echo=F}
Regional_Sites_AU <- filter(Regional_Sites_AU, !is.na(ID305B_1))
DT::datatable(Regional_Sites_AU, escape=F, rownames = F, options = list(scrollX = TRUE))
#View(Regional_Sites_AU)


conventionals_D_Region_AU <- rbind(conventionals_D_Region_AU, 
                                    Regional_Sites_AU %>% st_transform( st_crs(4326)), 
                                    conventionals_D_Region_NoAU %>% st_transform( st_crs(4326)))

rm(Regional_Sites_AU) # clean up workspace
```

