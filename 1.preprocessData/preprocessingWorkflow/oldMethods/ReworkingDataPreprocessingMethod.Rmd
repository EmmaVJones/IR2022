---
title: "Reworking Statewide Data Preprocessing"
author: "Emma Jones"
date: "2/18/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(sf)
library(readxl)
```


This project is a stream of consciousness walk through of how to rebuild the data preprocessing steps of the automated assessment methodology. 

## Data Organization

### Conventionals

This script was taken from the 2020 ambient network planning project. It combines the 2020IR conventionals data pull (2013-2018) with a special data pull for the ambient planning project that encompassed 2018-2019. 

```{r conventionals}
# Bring in 2020 IR data pull (2013-2018 data), will still need to bring in 2017&2018 sites
# Goal: filter out all BRRO sites to get a list of which sites were sampled each year
#  and frequency if possible
conventionals <- read_csv('C:/HardDriveBackup/R/GitHub/Rivers-StreamsAssessment/R&S_app_v4/data/final2020data/CEDSWQM_2020_IR_DATA-CONVENTIONALS_20190305.csv') 
conventionals$FDT_DATE_TIME2 <- as.POSIXct(conventionals$FDT_DATE_TIME, format="%m/%d/%Y %H:%M")

#summary(conventionals$FDT_DATE_TIME2)

# Now add recent data (2018-Nov 2019- the day Roger made the data pull)
# already limited to BRRO (SCRO and WCRO)
conventionals2 <- read_excel('C:/HardDriveBackup/R/GitHub/AmbientNetworkPlanning/for2020/data/CEDSWQM_CONVENTIONALS_2018+.xlsx')
conventionals2$FDT_DATE_TIME2 <- as.POSIXct(conventionals2$FDT_DATE_TIME, format="%m/%d/%Y %H:%M")
conventionals2$FDT_DATE_TIME <- as.character(conventionals2$FDT_DATE_TIME) # for smashing with original conventionals
summary(conventionals2$FDT_DATE_TIME2)

# filter data to just 2019 to not duplicate data from 2018
conventionals2019 <- filter(conventionals2, FDT_DATE_TIME2 > '2018-12-31 23:59:00')
summary(conventionals2019$FDT_DATE_TIME2)
# cool.
#glimpse(conventionals2019)

# what is in conventionals that isn't in conventionals2019??
names(conventionals)[!names(conventionals) %in% names(conventionals2019)]

conventionalsAll <- bind_rows(conventionals,conventionals2019) %>%
  # get groundwater sites out of here
  filter(FDT_SPG_CODE != 'GW')

conventionals <- filter(conventionalsAll, !is.na(Latitude)|!is.na(Longitude)) # remove sites without coordinates

rm(conventionalsAll);rm(conventionals2);rm(conventionals2019)
```

So we will use this as a pretend dataset to simuate what one could expect from 2022 IR, e.g. some attributed (AU and WQS) stations as well as some new stuff. This exercise will be conducted for BRRO, but the methods will apply statewide.

Before we can do any spatial subsetting work, we need to make a dataset of all UNIQUE StationID's for the regional offices to work with. 

```{r conventionalsDistinct, echo=FALSE}
conventionals_D <- distinct(conventionals, FDT_STA_ID, .keep_all = T) %>%
  select(FDT_STA_ID:FDT_SPG_CODE, Latitude:STA_CBP_NAME) # drop data to avoid any confusion

rm(conventionals) # remove full conventionals dataset to save memory

#View(conventionals_D) # to look at dataset use this command
```


### Citizen Data

Play with organizing citmon from the start this time?

Bring in James' raw dataset, knowing disaster lies ahead.  Dropping Group_Station_ID for now in hopes that user manipulation steps and better product from James can avoid that second name/layer of confusion.

```{r citmon}
# too big to read in using read_excel
cit <- read_csv('C:/HardDriveBackup/R/GitHub/Rivers-StreamsAssessment/CitizenNonAgency/2020IR Citizen Ambient4.14.19 (2).csv') %>%
  dplyr::select(Group_Station_ID:`DEQ QA Comments`) %>% # drop junk columns
  filter(!is.na(Latitude)|!is.na(Longitude)) %>% # drop sites without location information
  distinct(Group_Station_ID, FDT_STA_ID, Latitude, Longitude, .keep_all =T)  %>% #distinct by location and name
  dplyr::select(Group_Station_ID,FDT_STA_ID:FDT_SPG_CODE, Latitude:STA_CBP_NAME)# drop data to avoid confusion
```

### Non Agency

Add non agency data from the start this IR. 

Bring in James' raw dataset, knowing disaster lies ahead.  Dropping Group_Station_ID for now in hopes that user manipulation steps and better product from James can avoid that second name/layer of confusion.

```{r non agency}
# too big to read in using read_excel
# too big to read in using read_excel
nonA <- read_csv('C:/HardDriveBackup/R/GitHub/Rivers-StreamsAssessment/CitizenNonAgency/2020IR NONA ambient.csv')  %>%
  dplyr::select(Group_Station_ID:`DEQ QA Comments`) %>% # drop junk columns for now
  filter(!is.na(Latitude)|!is.na(Longitude)) %>% # drop sites without location information
  distinct(Group_Station_ID, FDT_STA_ID, Latitude, Longitude, .keep_all =T)  %>% #distinct by location and name
  dplyr::select(Group_Station_ID,FDT_STA_ID:FDT_SPG_CODE, Latitude:STA_CBP_NAME) # drop data to avoid confusion
```


### Single Input Dataset

By combining the conventionals, citmon, and non agency data, we can have a single starting point. We are also going to drop everything but station name and lat/lng as our joining information to avoid duplicating too much data.

```{r one input dataset}
irData <- bind_rows(cit, nonA) %>%
  bind_rows(conventionals_D) %>%
  dplyr::select(Group_Station_ID, FDT_STA_ID, Latitude, Longitude)

```


### Last Cycle Stations Table 2.0

Joining last cycle's station table 2.0 (station table with AU and WQS information) will save a lot of effort this year.

Since we are using BRRO as the testing region, we will bring in their final stations table 2.0 as a starting point. As this is scaled statewide we will smash all regions stations table 2.0 together as a starting point.

Since leftover 2018 IR results are still tucked in there we want to drop those columns to not confuse anyone.

```{r station table 2.0} 
RegionalResultsCombined <- read_csv('C:/HardDriveBackup/R/GitHub/Rivers-StreamsAssessment/R&S_app_v4/processedStationData/RegionalResultsRiverine_BRROCitMonNonAgencyFINAL.csv') %>%
  bind_rows(read_csv('C:/HardDriveBackup/R/GitHub/LakesAssessment2020/app2020/LakeAssessmentApp_citmon/processedStationData/lakeStations2020_BRRO_citmonNonAgency.csv')) %>%
  dplyr::select(FDT_STA_ID:VAHU6, Point.Unique.Identifier:Assess_TYPE) %>%
  mutate(Huc6_Huc_8 = paste0(0, Huc6_Huc_8))
```


## Initial Join: Free Metadata

So by using a simple join to existing information that has been QAed by a human, we can limit the amount of computational work and additional human QA.

```{r joinTime}
irData_BRRO <- filter(irData, str_detect(FDT_STA_ID,'9-'))

irData_join <- dplyr::left_join(irData_BRRO, RegionalResultsCombined,
                  by = c('FDT_STA_ID', 'Latitude', 'Longitude')) %>%
  st_as_sf(coords = c("Longitude", "Latitude"),  # make spatial layer using these columns
           remove = F, # don't remove these lat/lon cols from df
           crs = 4326) # add coordinate reference system, needs to be geographic for now bc entering lat/lng, 
```


## Just missing data from here

Now we will only deal with stations that did not join to the previous stations table by name, lat, and long. These stations are missing all CEDS attribute information, so that will be repopulated from the CEDS pull or James' notes.

```{r irData missing}
irData_missing <- filter(irData_join, is.na(ID305B_1)) # these had no previous records

irData_final <- filter(irData_join, !is.na(ID305B_1))

# make sure we didn't lose anyone
nrow(irData_final) + nrow(irData_missing) == nrow(irData_join)
```

Join back previously populated station information.


```{r repopulate station information}
irData_missing1 <- filter(cit, FDT_STA_ID %in% irData_missing$FDT_STA_ID) %>%
   bind_rows(filter(nonA, FDT_STA_ID %in% irData_missing$FDT_STA_ID)) %>%
  bind_rows(filter(conventionals_D, FDT_STA_ID %in% irData_missing$FDT_STA_ID))

rm(conventionals_D); rm(cit); rm(nonA)

```



## Assessment Units

### Statewide Assessment Layer

We are going to first associate a river basin with each station to make processing easier.

First, read in the statewide assessment layer.

```{r Statewide Assessment Layer, echo=FALSE, message=FALSE, warning=FALSE, results = 'hide'}
assessmentLayer <- st_read('GIS/AssessmentRegions_VA84_basins.shp') %>%
  st_transform( st_crs(4326)) # transform to WQS84 for spatial intersection
``` 

Then join to irData_join if the stations did not connect to the previous stations table.

```{r basin connection}
irData_missing1 <- st_join(irData_missing, assessmentLayer, join = st_intersects) %>%
  mutate(Basin = Basin.y,
         
    VAHU6 = ifelse(VAHU6 == Huc6_Vahu6, T, F)) %>% # double check the VAHU6 watershed matches from CEDS to actual spatial layer
  dplyr::select(FDT_STA_ID:STA_CBP_NAME, Basin, VAHU6, sameVAHU6)  # just get columns we are interested in, compare here if desired
```


