---
title: "Statewide Data Preprocessing"
author: "Emma Jones"
date: "5/6/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

library(tidyverse)
library(sf)
library(readxl)

```

This project is a outlines how to rebuild the automated preprocessing steps completed by assessment staff for 2020IR for the 2022IR with adjustments to incorporate updates to the WQS layer and WQS and AU attribution process.

Each step will be written as individual modules where lengthy code to complete each step is stored elsewhere and sourced for use. This method of organizing the project helps with project maintenance and troubleshooting specific problem areas when they occur.

# Water Quality Standards

## WQS Split up

After each update of WQS layers, it is important to split these layers into bite size consumption for the review app such that we don't send unnecessary data to/from servers and speed up rending time. All snapping will be completed against the official WQS layers, but for app purposes, we will only bring in WQS in a selected subbasin.

Because most (all) DEQ subbasin layers incorrectly attribute the upper Potomac basin inside the Shenandoah in VRO, we will create our own layer to work from that fixes this error. 

```{r fix subbasin layer}
#source('./preprocessingModules/newDEQsubbasinLayerCreation.R') # only run once as needed
```

That really only needs to be done once such that 'GIS/DEQsubbasins_EVJ.shp' is created. This is the layer the splitWQSbySubbasin will use.

Now we take that newly created layer and use it to split up each of the riverine, lakes, and both types of estuary layers appropriately.

```{r splitWQS by subbasin for app}
#source('./preprocessingModules/splitWQSbySubbasin.R') # only run once as needed
```

Now we can move on to actual data processing.


## Data Organization

The key data needed for these analyses is the conventionals dataset that Roger Stewart pulls every two years for each IR window. This method is built such that additional stations that need WQS data attached (for EDAS/CEDS Stations Table update project) can be run through just that portion of the methodology.


### Conventionals Data

This version of the method smashed together a fully QAed version of the conventionals dataset (2020IR) and a not fully QAed dataset from 2019 pulled using Roger's conventionals methods. For 2022 final dataset, a single conventional dataset will be brought in.

#### Conventionals data smashing 2013-2018 + 2019

```{r conventionals smash}
#source('./preprocessingModules/conventionalsSmashing2013-2019.R')
```

So we will use this as a pretend dataset to simuate what one could expect from 2022 IR, e.g. some attributed (AU and WQS) stations as well as some new stuff. This exercise will be conducted for BRRO, but the methods will apply statewide.

### Other data

Stations pulled from CEDS with Lat/Lng info could be run through the WQS steps if uploaded and reorganized here.

Cross that bridge when we get there...


### Find Distinct Sites

No matter where you are pulling data in from, you need to limit these computationally heavy spatial analyses to one per site. Running a distinct on the dataset and making it a spatial dataset is the best next step.

```{r distinct sites}
#conventionals_D <- distinct(conventionals, FDT_STA_ID, .keep_all = T) %>%
#  select(FDT_STA_ID:FDT_SPG_CODE, Latitude:STA_CBP_NAME, -FDT_DATE_TIME) # drop data to avoid any confusion 
#rm(conventionals) # remove full conventionals dataset to save memory

distinctSites <- readRDS('./data/conventionals_D.RDS') # for some reason doesn't work in chunk but does in console
```

### Attach assessment region and subbasin information

Assessment region is important for processing each region through a loop, AU connection, and WQS attachment. Subbasin information is important for WQS processing.

```{r assessment and subbasin join}
source('preprocessingModules/assessmentLayerSubbasinJoin.R') # for some reason doesn't work in chunk but does in console
distinctSitesToDo <- distinctSites_sf 
# keep a copy of original distinct sites to check that no one was lost at the end
``` 

**Note the VAHU6 data is derived spatially, so this is a good first step, but when there is human QAed VAHU6 data available, we will use that data instead. Sometimes assessors use stations outside a VAHU6 to assess said VAHU6.**

### Spatially Join WQS

Since WQS is in transition to new system, we first need to do all spatial joins to new WQS layer to get appropriate UID information, then we can use human QAed info from previous assessment (where available) to double check the standards snapped correctly.

Here is the table used to store link information from stations to appropriate WQS.

```{r WQStable}
WQStable <- tibble(StationID = NA, WQS_ID = NA)

# in future bring in existing table
```



Since it is much faster to look for spatial joins by polygons compared to snapping to lines, we will run spatial joins by estuarine polys and reservoir layers first.

##### Estuarine Polygons

Find any sites that fall into an estuary WQS polygon. This method is only applied to subbasins that intersect estuarine areas.
Removes any estuarine sites from the data frame of unique sites that need WQS information.

```{r estuary methods}
source('preprocessingModules/WQS_estuaryPoly.R')

# Bring in estuary layer
estuarinePolys <- st_read('GIS/WQS_layers_05082020.gdb', layer = 'estuarinepolygons_05082020' , fid_column_name = "OBJECTID") %>%
  st_transform(4326)
  
WQStable <- estuaryPolygonJoin(estuarinePolys, distinctSitesToDo, WQStable)

rm(estuarinePolys) # clean up workspace
```

Remove stations that fell inside estuarine polygons from the 'to do' list.

```{r remove estuary poly sites}
distinctSitesToDo <- filter(distinctSitesToDo, ! FDT_STA_ID %in% WQStable$StationID)
```


##### Lake Polygons

Find any sites that fall into a lake WQS polygon. This method is applied to all subbasins.
Removes any lake sites from the data frame of unique sites that need WQS information.


```{r lake methods}
source('preprocessingModules/WQS_lakePoly.R')

lakesPoly <- st_read('GIS/WQS_layers_05082020.gdb', layer = 'lakes_reservoirs_05082020' , fid_column_name = "OBJECTID") %>%
  st_transform(4326)

WQStable <- lakePolygonJoin(lakesPoly, distinctSitesToDo, WQStable)

rm(lakesPoly) # clean up workspace
```

Remove stations that fell inside lake polygons from the 'to do' list.

```{r remove estuary poly sites}
distinctSitesToDo <- filter(distinctSitesToDo, ! FDT_STA_ID %in% WQStable$StationID)
```



### Spatially Join WQS Lines

Now on to the more computationally heavy WQS line snapping methods. First we will try to attach riverine WQS, and where stations remain we will try the estuarine WQS snap.

##### Riverine Lines

Buffer all sites that don't fall into a polygon layer. The output will add a field called `Buffer Distance` to the WQStable to indicate distance required for snapping. This does not get transported to the data of record, but it is useful to keep for now for QA purposes. If more than one segment is found within a set buffer distance, that many rows will be attached to the WQStable with the identifying station name. It is up to the QA tool to help the user determine which of these UID's are correct and drop the other records.

Removes any riverine sites from the data frame of unique sites that need WQS information.

```{r riverine methods}
source('snappingFunctions/snapWQS.R')

riverine <- st_read('GIS/WQS_layers_05082020.gdb', layer = 'riverine_05082020' , fid_column_name = "OBJECTID") %>%
  st_transform(102003) # forcing to albers from start bc such a huge layer
  #st_transform(4326)

WQStable <- snapAndOrganizeWQS(distinctSitesToDo, 'FDT_STA_ID', riverine, 
                               bufferDistances = seq(20,80,by=20),  # buffering by 20m from 20 - 80 meters
                               WQStable)


rm(riverine)
```

Remove stations that attached to riverine segments from the 'to do' list.

```{r remove estuary poly sites}
distinctSitesToDo <- filter(distinctSitesToDo, ! FDT_STA_ID %in% WQStable$StationID)
```


For sites that did not snap to any riverine segments, we need to test against the estuarine WQS segments.

```{r no riverine snaps}
distinctSitesToDo <- filter(WQStable, `Buffer Distance` =='No connections within 80 m') %>%
  left_join(dplyr::select(distinctSites_sf, FDT_STA_ID, Latitude, Longitude, SUBBASIN) %>%
              rename('StationID'= 'FDT_STA_ID'), by='StationID') %>%
  dplyr::select(-c(geometry)) %>%
  st_as_sf(coords = c("Longitude", "Latitude"),  # make spatial layer using these columns
           remove = T, # don't remove these lat/lon cols from df
           crs = 4326)
                            
```



##### Estuarine Lines

If a site doesn't attach to a riverine segment, our last step is to try to attach estuary line segments before throwing an empty site to the users for the wild west of manual QA. Only send sites that could be in estuarine subbasin to this function to not waste time.
Removes any estuary lines sites from the data frame of unique sites that need WQS information.

```{r estuarine lines methods}
source('snappingFunctions/snapWQS.R')

estuarineLines <- st_read('GIS/WQS_layers_05082020.gdb', layer = 'estuarinelines_05082020' , fid_column_name = "OBJECTID") %>%
  st_transform(102003) # forcing to albers from start bc such a huge layer
  #st_transform(4326)

# Only send sites to function that could be in estuarine environment
WQStable <- snapAndOrganizeWQS(filter(distinctSitesToDo, SUBBASIN %in% c("Potomac River", "Rappahannock River", 
                                                                         "Atlantic Ocean Coastal", "Chesapeake Bay Tributaries",
                                                                         "Chesapeake Bay - Mainstem", "James River - Lower",  
                                                                         "Appomattox River","Chowan River", 
                                                                         "Atlantic Ocean - South" , "Dismal Swamp/Albemarle Sound"))[1:25,],
                               'StationID', estuarineLines, 
                               bufferDistances = seq(20,80,by=20),  # buffering by 20m from 20 - 80 meters
                               WQStable)

rm(estuarineLines)
```

Remove stations that attached to riverine segments from the 'to do' list.

```{r remove estuary poly sites}
distinctSitesToDo <- filter(distinctSitesToDo, ! StationID %in% WQStable$StationID)
```


Make sure all stations from original distinct station list have some sort of record (blank or populated) int the WQStable.

```{r double check no one lost}
distinctSites$FDT_STA_ID[!(distinctSites$FDT_STA_ID %in% unique(WQStable$StationID))]
```

And that none of the missing sites (sites that fall outside of the assessmentLayer) are not missing from the WQS snapping process.

```{r missingSites got WQS}
missingSites$FDT_STA_ID[!(missingSites$FDT_STA_ID %in% unique(WQStable$StationID))]
```



### QA Spatial Attribution Process

See how many sites snapped to too many WQS segments

```{r snapCheck}
tooMany <- snapCheck(WQStable)

fine <- filter(WQStable, ! (StationID %in% tooMany$StationID)) %>% 
  filter(`Buffer Distance` %in% c(NA,  "20 m", "40 m", "60 m" ,"80 m"))
none <- filter(WQStable, ! StationID %in% tooMany$StationID) %>%
  filter(`Buffer Distance` == "No connections within 80 m")

# quick stats 
(nrow(fine) / nrow(distinctSites)) * 100 # ~ 74% snapped to one segment
(length(unique(tooMany$StationID)) / nrow(distinctSites)) * 100 # ~ 23% need extra manual review
(nrow(none) / nrow(distinctSites)) * 100 # ~ 4% snapped to one segment

rm(fine);rm(none);rm(tooMany)
```





### Attach any free data available from previous assessment cycles

This data is smashed together from the regions who returned their preprocessing steps from the 2020IR. This procedure also updates existing WQS designations to updated WQS.

```{r 2020IR preprocessing data statewide}
source('preprocessingModules/2020IRpreprocessingDataStatewide.R')

WQStableCheck <- filter(WQStable, StationID %in% existingData$FDT_STA_ID)
```


### Use UID to attach standards for sites were existing data occurs

```{r get standards based on UID}
source('preprocessingModules/wqsQA.R')
WQStableChecked <- humanQA(WQStableCheck)
```
So the above object WQStableChecked isn't perfect in that where we have a site that has human QAed data we now have the associated WQS_ID, but we have weeded out sites that don't have the same WQS as the human QAed sites. There are still duplicated sites in this dataset for a human to go through.

That QA step got us practically nowhere so saving out the WQStable object as the official dataset for application building.
```{r data for app}
saveRDS(WQStable, 'data/processedWQS/WQStable.RDS')
write.csv(WQStable, 'data/processedWQS/WQStable.csv', row.names = F)
```





#-------------------------------------------------------------------------------------------------------------------------------------------------------------








# Assessment Unit

## Split AUs for application easy rendering

First we need to take the spatial data we will use for the app and split it efficiently.


**DRAFT 2020 IR spatial data here, need to rerun with final data release!!!!!**

```{r split AUs for app}
# only run once
#source('preprocessingModules/splitAUbySubbasin.R')
```




## Assessment Unit Info from Last Cycle

The logical starting point is to take all the unique stations that need to be assessed (distinctSites_sf from above) and join AU information by StationID where possible before going to more computationally intensive methods. 

Bring in Cleo's final stations data from 2020 cycle.

```{r cleo 2020 final stations}
final2020 <- st_read('GIS/2020_wqms.shp') %>%
  st_transform(4326)
```

Join distinct sites to AU information to get all available free data.

```{r AU join}

# start by adding back in missing sites (sites that are outside assessmentLayer boundary)
missingSites_AU <- mutate(missingSites, HUC12 = NA, VAHU6 = NA, Portion = NA, MAP = NA, ASSESS_REG = NA, OFFICE_NM = NA, States = NA, HUType = NA, HUMod = NA, ToHUC = NA, META_ID = NA, Location = NA, VaName = NA, PC_Water = NA, Tidal = NA, VAHUSB = NA, FedName = NA, HUC10 = NA, VAHU5 = NA, Basin = NA, BASIN_NAME = NA, BASIN_CODE = NA, SUBBASIN = NA) %>%
  dplyr::select(names(distinctSites_sf)) %>% st_drop_geometry()


distinctSites_AUall <- distinctSites_sf %>% 
  st_drop_geometry() %>%
  rbind(missingSites_AU) %>%
  left_join(final2020 %>% st_drop_geometry(), # bc two spatial objects need to drop geometry to join on tabular data
                              by = c('FDT_STA_ID' = 'STATION_ID')) %>%
  dplyr::select(FDT_STA_ID : VAHU6.y) %>% # drop the last cycle's results, not important now
  mutate(VAHU6 = ifelse(is.na(VAHU6.y), as.character(VAHU6.x), as.character(VAHU6.y))) %>% # use last cycle's VAHU6 designation over CEDS designation by default if available
  dplyr::select(-c(VAHU6.x, VAHU6.y)) %>%
  group_by(FDT_STA_ID) %>%
  mutate(n = n())

# Find any duplicates
View(filter(distinctSites_AUall, n >1)) # makes sense, these sites are being used for riverine and lacustrine assessment
```

Organize stations by whether or not they have AU data.

```{r AU haves and have nots}
# Needs Work
distinctSites_AUtoDo <- filter(distinctSites_AUall, is.na(ID305B_1)) %>%
  st_as_sf(coords = c("Longitude", "Latitude"),  # make spatial layer using these columns
           remove = T, # don't remove these lat/lon cols from df
           crs = 4326)

# All good for assessment app (once we join WQS info from lookup table)
distinctSites_AU <- filter(distinctSites_AUall, !is.na(ID305B_1)) %>%
  st_as_sf(coords = c("Longitude", "Latitude"),  # make spatial layer using these columns
           remove = T, # don't remove these lat/lon cols from df
           crs = 4326)

nrow(distinctSites_AU) + nrow(distinctSites_AUtoDo) == nrow(distinctSites_AUall)
```

## Spatially Join AU information

Since it is much faster to look for spatial joins by polygons compared to snapping to lines, we will run spatial joins by estuarine polys and reservoir layers first.

##### Estuarine Polygons AU

Find any sites that fall into an estuary AU polygon. This method is only applied to subbasins that intersect estuarine areas.
Removes any estuarine sites from the data frame of unique sites that need AU information.


**This is DRAFT 2020 IR AU SPATIAL DATA, Needs to potentially be rerun with FINAL 2020 IR SPATIAL DATA**

```{r estuary methods AU}
source('preprocessingModules/AU_Poly.R')

# Bring in estuary layer
estuaryPolysAU <- st_read('C:/HardDriveBackup/GIS/Assessment/2020IR_draft/va_20ir_aus.gdb', layer = 'va_2020_aus_estuarine' , 
                          fid_column_name = "OBJECTID") %>%
   st_transform( 4326 ) %>%
   st_cast("MULTIPOLYGON") # special step for weird WKB error reading in geodatabase, never encountered before, fix from: https://github.com/r-spatial/sf/issues/427
  
estuaryPolysAUjoin <- polygonJoinAU(estuaryPolysAU, distinctSites_AUtoDo, estuaryTorF = T) %>%
  mutate(ID305B_1 = ID305B) %>%
  dplyr::select(names(distinctSites_AU)) %>%
  group_by(FDT_STA_ID) %>%
  mutate(n = n()) %>%
  ungroup() 

rm(estuaryPolysAU) # clean up workspace
```

Add Estuary stations to distinctSites_AU.

```{r add estuary AU sites}
distinctSites_AU <- rbind(distinctSites_AU, estuaryPolysAUjoin)
```


Remove stations that fell inside estuarine polygons from the 'to do' list.

```{r remove estuary poly sites AU}
distinctSites_AUtoDo <- filter(distinctSites_AUtoDo, ! FDT_STA_ID %in% estuaryPolysAUjoin$FDT_STA_ID)

nrow(distinctSites_AU) + nrow(distinctSites_AUtoDo) == nrow(distinctSites_AUall)
```


##### Lake Polygons

Find any sites that fall into a lake AU polygon. This method is applied to all subbasins.
Removes any lake sites from the data frame of unique sites that need AU information.

**This is DRAFT 2020 IR AU SPATIAL DATA, Needs to potentially be rerun with FINAL 2020 IR SPATIAL DATA**


```{r lake methods AU}
source('preprocessingModules/AU_Poly.R')

# Bring in Lakes layer
lakesPolyAU <- st_read('C:/HardDriveBackup/GIS/Assessment/2020IR_draft/va_20ir_aus.gdb', layer = 'va_2020_aus_reservoir' , 
                          fid_column_name = "OBJECTID") %>%
   st_transform( 4326 ) %>%
   st_cast("MULTIPOLYGON") # special step for weird WKB error reading in geodatabase, never encountered before, fix from: https://github.com/r-spatial/sf/issues/427

lakesPolysAUjoin <- polygonJoinAU(lakesPolyAU, distinctSites_AUtoDo, estuaryTorF = T)%>%
  mutate(ID305B_1 = ID305B) %>%
  dplyr::select(names(distinctSites_AU)) %>%
  group_by(FDT_STA_ID) %>%
  mutate(n = n()) %>%
  ungroup() 

rm(lakesPolyAU) # clean up workspace
```

Add lake stations to distinctSites_AU.

```{r add estuary AU sites}
distinctSites_AU <- rbind(distinctSites_AU, lakesPolysAUjoin)
```



Remove stations that fell inside lake polygons from the 'to do' list.

```{r remove estuary poly sites}
distinctSites_AUtoDo <- filter(distinctSites_AUtoDo, ! FDT_STA_ID %in% lakesPolysAUjoin$FDT_STA_ID)

nrow(distinctSites_AU) + nrow(distinctSites_AUtoDo) == nrow(distinctSites_AUall)


rm(lakesPolysAUjoin);rm(estuaryPolysAUjoin)
```



### Spatially Join AU Lines

Now on to the more computationally heavy AU line snapping methods. First we will try to attach riverine AUs, and where stations remain we will try the estuarine lines AU snap.

##### Riverine Lines AU

Buffer all sites that don't fall into a polygon layer. The output will add a field called `Buffer Distance` to the distinctSites_AU to indicate distance required for snapping. This does not get transported to the data of record, but it is useful to keep for now for QA purposes. If more than one segment is found within a set buffer distance, that many rows will be attached to the WQStable with the identifying station name. It is up to the QA tool to help the user determine which of these AU's are correct and drop the other records.

Removes any riverine sites from the data frame of unique sites that need AU information.

**This is DRAFT 2020 IR AU SPATIAL DATA, Needs to potentially be rerun with FINAL 2020 IR SPATIAL DATA**


```{r riverine methods AU}
source('snappingFunctions/snapPointToStreamNetwork.R')

riverineAU <- st_read('C:/HardDriveBackup/GIS/Assessment/2020IR_draft/va_2020_aus_riverine.shp') %>%
    st_transform(102003) # forcing to albers from start bc such a huge layer   
  
  # Even the Edzer hack didn't work on riverine layers so exported draft riverine layer to shapefile for now. Hopefully final dataset comes as decent .gdb
  #st_read('C:/HardDriveBackup/GIS/Assessment/2020IR_draft/va_20ir_aus.gdb', layer = 'va_2020_aus_riverine' , 
              #            fid_column_name = "OBJECTID") %>%
  #st_transform(102003) %>% # forcing to albers from start bc such a huge layer   
  #st_cast("MULTILINESTRING") # special step for weird WKB error reading in geodatabase, never encountered before, fix from: https://github.com/r-spatial/sf/issues/427


snapTable <- snapAndOrganize(distinctSites_AUtoDo, 'FDT_STA_ID', riverineAU, 
                             bufferDistances = seq(20,80,by=20),  # buffering by 20m from 20 - 80 meters
                             tibble(StationID = character(), ID305B = character(), `Buffer Distance` = character()),
                             "ID305B")

#snapTable <- readRDS('preprocessingWorkflow/snapTable.RDS')

snapTable <- snapTable %>%
  left_join(distinctSites_AUtoDo, by = c('StationID' = 'FDT_STA_ID')) %>% # get station information
  rename('FDT_STA_ID' = 'StationID') %>%
  mutate(ID305B_1 = ID305B) %>%
  dplyr::select(names(distinctSites_AU), `Buffer Distance`) %>%
  group_by(FDT_STA_ID) %>%
  mutate(n = n()) %>%
  ungroup()
  
  
rm(riverineAU)
```

Add these sites to the sites with AU information.

```{r add to AU table}
distinctSites_AU <- bind_rows(distinctSites_AU %>% ungroup() %>% st_drop_geometry(), snapTable)
```

Remove stations that attached to riverine segments from the 'to do' list.

```{r remove riverine snapped AU sites}
distinctSites_AUtoDo <- filter(distinctSites_AUtoDo, ! FDT_STA_ID %in% snapTable$FDT_STA_ID)
```


We don't have estuarine lines AU information, so the sites that don't connect to any AU's at the max buffer distance will have to be sorted out by the assessors.



Make sure all stations from original distinct station list have some sort of record (blank or populated) in the distinctSites_AU dataset.

```{r double check no one lost AU}

# check everyone dealt with
nrow(distinctSites_AU) + nrow(distinctSites_AUtoDo) == nrow(distinctSites_AUall)

distinctSites_AU$FDT_STA_ID[!(distinctSites_sf$FDT_STA_ID %in% unique(distinctSites_AU$FDT_STA_ID))]

if(nrow(distinctSites_AUtoDo) == 0){rm(distinctSites_AUtoDo)}

#View(filter(distinctSites_sf, FDT_STA_ID %in% distinctSites_AU$FDT_STA_ID[!(distinctSites_sf$FDT_STA_ID %in% unique(distinctSites_AU$FDT_STA_ID))]))
```

And that none of the missing sites (sites that fall outside of the assessmentLayer) are not missing from the AU snapping process.

```{r missingSites got WQS}
missingSites$FDT_STA_ID[!(missingSites$FDT_STA_ID %in% unique(distinctSites_AU$FDT_STA_ID))]
```

Save your work!

```{r sites ready for app review}
write.csv(distinctSites_AU %>% dplyr::select(-geometry), './data/preAnalyzedAUdata.csv', row.names = F)
```

