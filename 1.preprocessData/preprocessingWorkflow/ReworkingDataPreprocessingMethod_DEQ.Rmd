---
title: "Reworking Statewide Data Preprocessing"
author: "Emma Jones"
date: "2/18/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(sf)
library(readxl)
library(miniUI)
library(shiny)
library(leaflet)
library(mapview)

source('snappingFunctions/snapFunctions.R') # snapping functions
source('snappingFunctions/snapOrganizationFunctions_v2.R') # functions to do stuff with snapping functions
source('snappingFunctions/snapOrganizationFunctions_messAround.R')
```


This project is a stream of consciousness walk through of how to rebuild the data preprocessing steps of the automated assessment methodology. 

## Data Organization

### Conventionals

This script was taken from the 2020 ambient network planning project. It combines the 2020IR conventionals data pull (2013-2018) with a special data pull for the ambient planning project that encompassed 2018-2019. 

```{r conventionals}
# Bring in 2020 IR data pull (2013-2018 data), will still need to bring in 2017&2018 sites
# Goal: filter out all BRRO sites to get a list of which sites were sampled each year
#  and frequency if possible
conventionals <- read_csv('C:/HardDriveBackup/R/GitHub/Rivers-StreamsAssessment/R&S_app_v4/data/final2020data/CEDSWQM_2020_IR_DATA-CONVENTIONALS_20190305.csv') 
conventionals$FDT_DATE_TIME2 <- as.POSIXct(conventionals$FDT_DATE_TIME, format="%m/%d/%Y %H:%M")

#summary(conventionals$FDT_DATE_TIME2)

# Now add recent data (2018-Nov 2019- the day Roger made the data pull)
# already limited to BRRO (SCRO and WCRO)
conventionals2 <- read_excel('C:/HardDriveBackup/R/GitHub/AmbientNetworkPlanning/for2020/data/CEDSWQM_CONVENTIONALS_2018+.xlsx')
conventionals2$FDT_DATE_TIME2 <- as.POSIXct(conventionals2$FDT_DATE_TIME, format="%m/%d/%Y %H:%M")
conventionals2$FDT_DATE_TIME <- as.character(conventionals2$FDT_DATE_TIME) # for smashing with original conventionals
summary(conventionals2$FDT_DATE_TIME2)

# filter data to just 2019 to not duplicate data from 2018
conventionals2019 <- filter(conventionals2, FDT_DATE_TIME2 > '2018-12-31 23:59:00')
summary(conventionals2019$FDT_DATE_TIME2)
# cool.
#glimpse(conventionals2019)

# what is in conventionals that isn't in conventionals2019??
names(conventionals)[!names(conventionals) %in% names(conventionals2019)]

conventionalsAll <- bind_rows(conventionals,conventionals2019) %>%
  # get groundwater sites out of here
  filter(FDT_SPG_CODE != 'GW')

conventionals <- filter(conventionalsAll, !is.na(Latitude)|!is.na(Longitude)) # remove sites without coordinates

rm(conventionalsAll);rm(conventionals2);rm(conventionals2019)
```

So we will use this as a pretend dataset to simuate what one could expect from 2022 IR, e.g. some attributed (AU and WQS) stations as well as some new stuff. This exercise will be conducted for BRRO, but the methods will apply statewide.

Before we can do any spatial subsetting work, we need to make a dataset of all UNIQUE StationID's for the regional offices to work with. 

```{r conventionalsDistinct, echo=FALSE}
conventionals_D <- distinct(conventionals, FDT_STA_ID, .keep_all = T) %>%
  select(FDT_STA_ID:FDT_SPG_CODE, Latitude:STA_CBP_NAME) # drop data to avoid any confusion

rm(conventionals) # remove full conventionals dataset to save memory


#View(conventionals_D) # to look at dataset use this command
```


Make conventionals dataset by basin for app. Run once each cycle

```{r Statewide Assessment Layer, echo=FALSE, message=FALSE, warning=FALSE, results = 'hide'}
assessmentLayer <- st_read('GIS/AssessmentRegions_VA84_basins.shp') %>%
  st_transform( st_crs(4326)) # transform to WQS84 for spatial intersection
``` 

```{r, eval=FALSE}

#conventionals_D_basin <- st_as_sf(conventionals_D,
#                        coords = c("Longitude", "Latitude"),  # make spatial layer using these columns
#                        remove = F, # don't remove these lat/lon cols from df
#                        crs = 4326) %>% 
#  st_join(dplyr::select(assessmentLayer, ASSESS_REG, Basin), join = st_intersects)  # intersect last cycle's lake AUs with assessmentLayer to get appropriate Basin #argument for data organization
  
#conventionals_D_basinNA <- filter(conventionals_D_basin, is.na(Basin)) %>% 
#  mutate(Basin, Basin = case_when(grepl("^1", as.character(FDT_STA_ID)) ~ "Potomac River Basin",
#                                  grepl("^2", as.character(FDT_STA_ID)) ~ "James River Basin",
#                                  grepl("^3", as.character(FDT_STA_ID)) ~ "Rappahannock River Basin",
#                                  grepl("^4", as.character(FDT_STA_ID)) ~ "Roanoke River Basin",
#                                  grepl("^5", as.character(FDT_STA_ID)) ~ "Chowan and Dismal Swamp River Basin",
#                                  grepl("^6", as.character(FDT_STA_ID)) ~ "Tennessee and Big Sandy River Basin",
#                                  grepl("^7", as.character(FDT_STA_ID)) ~ "Ches. Bay and Small Coastal Basin",
#                                  grepl("^8", as.character(FDT_STA_ID)) ~ "York River Basin",
#                                  grepl("^9", as.character(FDT_STA_ID)) ~ "New River Basin"))# some fixes

### FLAG IN CASE STILL DOESNT WORK
#View(filter(conventionals_D_basinNA, is.na(Basin))) # deal with these later

#conventionals_D_basin <- filter(conventionals_D_basin, !(FDT_STA_ID %in% conventionals_D_basinNA$FDT_STA_ID)) %>%
#  rbind(conventionals_D_basinNA) %>%
#  filter(!is.na(Basin))

#for(i in 1:length(unique(conventionals_D_basin$Basin))){
#  print(i)
#  z <- filter(conventionals_D_basin, Basin == as.character(unique(conventionals_D_basin$Basin)[i]))
#  st_write(z, paste0('data/conventionals_D_', unique(z$Basin), '.shp'))
#}

  
## Save for app, old method
##st_write(conventionals_D %>% st_as_sf(coords = c("Longitude", "Latitude"),  # make spatial layer using these #columns
##           remove = F, # don't remove these lat/lon cols from df
##           crs = 4326), # add coordinate reference system, needs to be geographic for now bc entering lat/lng, 
##         'GIS/conventionals_D.shp')

```



### Single Input Dataset

By combining the conventionals, citmon, and non agency data, we can have a single starting point. We are also going to drop everything but station name and lat/lng as our joining information to avoid duplicating too much data.

```{r one input dataset}
irData <-# bind_rows(cit, nonA) %>%
  #bind_rows(conventionals_D) %>%
  #dplyr::select(Group_Station_ID, FDT_STA_ID, Latitude, Longitude)
  mutate(conventionals_D, UID = row_number()) %>% # create a UID for QA purposes
  dplyr::select(UID, FDT_STA_ID, Latitude, Longitude)


```


### Last Cycle Stations Table 2.0

Joining last cycle's station table 2.0 (station table with AU and WQS information) will save a lot of effort this year.

Since we are using BRRO as the testing region, we will bring in their final stations table 2.0 as a starting point. As this is scaled statewide we will smash all regions stations table 2.0 together as a starting point.

Since leftover 2018 IR results are still tucked in there we want to drop those columns to not confuse anyone.

```{r station table 2.0} 
RegionalResultsCombined <- read_csv('C:/HardDriveBackup/R/GitHub/Rivers-StreamsAssessment/R&S_app_v4/processedStationData/RegionalResultsRiverine_BRROCitMonNonAgencyFINAL.csv') %>%
  bind_rows(read_csv('C:/HardDriveBackup/R/GitHub/LakesAssessment2020/app2020/LakeAssessmentApp_citmon/processedStationData/lakeStations2020_BRRO_citmonNonAgency.csv')) %>%
  dplyr::select(FDT_STA_ID:VAHU6, Point.Unique.Identifier:Assess_TYPE) %>%
  mutate(Huc6_Huc_8 = paste0(0, Huc6_Huc_8))
```


## Bring in Subbasin information (mainly for WQS work)

Read in the statewide subbasin layer.

```{r Statewide subbasin layer, echo=FALSE, message=FALSE, warning=FALSE, results = 'hide'}
subbasinLayer <- st_read('GIS/deq_basins07.shp') %>%
  mutate(BASIN_CODE = case_when(BASIN_CODE == '3-' ~ '3',
                                BASIN_CODE == '8-' ~ '8',
                                BASIN_CODE == '9-' ~ '9',
                                TRUE ~ as.character(BASIN_CODE)),
         SUBBASIN = case_when(BASIN_CODE == '3' ~ 'Rappahannock River',
                              BASIN_CODE == '8-' ~ 'York River',
                              BASIN_CODE == '9' ~ 'New River', 
                              TRUE ~ as.character(SUBBASIN))) %>%
  st_transform( st_crs(4326)) # transform to WQS84 for spatial intersection
``` 



## Initial Join: Free Metadata

So by using a simple join to existing information that has been QAed by a human, we can limit the amount of computational work and additional human QA.



Choose an assessment region
```{r}
assessmentRegion <- unique(assessmentLayer$ASSESS_REG)[1]
```



```{r joinTime}
#james in BRRO bc first thing that comes up in app
irData_BRRO <- st_as_sf(irData,
                        coords = c("Longitude", "Latitude"),  # make spatial layer using these columns
                        remove = F, # don't remove these lat/lon cols from df
                        crs = 4326) %>% # add coordinate reference system, needs to be geographic for now bc entering lat/lng, 
  st_intersection(filter(assessmentLayer, ASSESS_REG == assessmentRegion) ) %>%
  st_join(dplyr::select(subbasinLayer, BASIN_NAME, BASIN_CODE, SUBBASIN), join = st_intersects)

irData_join <- dplyr::left_join(irData_BRRO, 
                                # but first drop columns that we want from spatial layer
                                dplyr::select(RegionalResultsCombined,
                                              -c(Basin, OFFICE_NM, Location, Shape_Leng)),
                  by = c('FDT_STA_ID', 'Latitude', 'Longitude')) %>% 
  # Fix duplicate columns
  mutate(VAHU6 = ifelse(is.na(VAHU6.y), as.character(VAHU6.x), as.character(VAHU6.x))) %>% # keep basin from assessor call if exists
  # spatial join could accidentally replace when assessor uses site to assess VAHU6 that it doesn't fall in
  dplyr::select(-c(VAHU6.x, VAHU6.y)) %>%
  dplyr::select(UID, FDT_STA_ID, Latitude, Longitude, HUC12, VAHU6, everything())
    
  
# 9-PCK004.XX is duplicated bc lake and riverine assessment info attached, so nrow goes up by 1 at join step
```



## Just missing data from here

Now we will only deal with stations that did not join to the previous stations table by name, lat, and long. These stations are missing all CEDS attribute information, so that will be repopulated from the CEDS pull or James' notes.

```{r irData missing}
irData_missing <- filter(irData_join, is.na(ID305B_1)) # these had no previous records

irData_final <- filter(irData_join, !is.na(ID305B_1))

# make sure we didn't lose anyone
nrow(irData_final) + nrow(irData_missing) == nrow(irData_join)
```

Join back previously populated station information.


```{r repopulate station information}
irData_missing <- #filter(cit, FDT_STA_ID %in% irData_missing$FDT_STA_ID) %>%
   #bind_rows(filter(nonA, FDT_STA_ID %in% irData_missing$FDT_STA_ID)) %>%
  #bind_rows(filter(conventionals_D, FDT_STA_ID %in% irData_missing$FDT_STA_ID))
  left_join(dplyr::select(irData_missing, UID:Longitude), 
            conventionals_D, by = c('FDT_STA_ID','Latitude', 'Longitude')) %>%
  dplyr::select(-geometry,geometry)

#rm(conventionals_D)#; rm(cit); rm(nonA) # keep for WQS work

```



## Assessment Units

### Statewide Assessment Layer

We are going to first associate a river basin with each station to make processing easier.

First, read in the statewide assessment layer.

```{r Statewide Assessment Layer, echo=FALSE, message=FALSE, warning=FALSE, results = 'hide'}
assessmentLayer <- st_read('GIS/AssessmentRegions_VA84_basins.shp') %>%
  st_transform( st_crs(4326)) # transform to WQS84 for spatial intersection
``` 

Then join to irData_join if the stations did not connect to the previous stations table.

```{r basin connection, eval=F}
irData_missing1 <- st_join(irData_missing, assessmentLayer, join = st_intersects) %>%
    #filter(!is.na(VAHU6)) %>% # only keep rows that joined
  mutate(sameVAHU6 = ifelse(VAHU6 == Huc6_Vahu6, T, F)) %>% # double check the VAHU6 watershed matches from CEDS to actual spatial layer
  dplyr::select(UID:STA_CBP_NAME, Basin, VAHU6, sameVAHU6) %>% # just get columns we are interested in, compare here if desired
  dplyr::select(UID:STA_CBP_NAME,Basin)
```


#### Last Cycle Estuary Assessment Layer

First thing we want to do now is filter out any sites that fall into the estuary AUs. These stations will have to be assessed through other means. **Note: you can download the table below for reference by running the commented script. These are all the stations within the selected assessment region that fall into an estuary AU based on the 2018 AU layers.**

The below chunk:
1) Reads in the 2018 estuary AU layer
1.1) intersects assessmentLayer to get appropriate basin info
1.2) saves each layer in a spot to be accessed by app

This needs to be rerun each cycle with the most recent final AU information

```{r Filter Out Estuary Statewide, echo=F, results = 'hide'}
#estuaryAUs <- st_read('C:/HardDriveBackup/GIS/Assessment/va_2018_aus_estuarine.shp') %>%
#   st_transform( st_crs(4326)) %>% # transform to WQS84 for spatial intersection 
#  # intersect last cycle's lake AUs with assessmentLayer to get appropriate Basin argument for data organization
#   st_join(dplyr::select(assessmentLayer, ASSESS_REG, Basin), join = st_intersects) 

#for(i in 1:length(unique(estuaryAUs$ASSESS_REG))){
#  z <- filter(estuaryAUs, ASSESS_REG == as.character(unique(estuaryAUs$ASSESS_REG)[i]))
#  for(k in 1:length(unique(z$Basin))){
#    z1 <- filter(z, Basin == as.character(unique(z$Basin)[k]))
#    st_write(z1, paste0('data/processedGIS/va_2018_aus_estuarine_', 
#                          unique(z1$ASSESS_REG), "_", 
#                          unique(z1$Basin), '.shp'))
#  }
#}


```

2) Finds any sites that fall into an estuary AU polygon
  - see optional code to write those sites to a separate csv for future reference
3) Removes any estuarine sites from the data frame of unique conventionals sites that need AU and WQS information.

```{r Filter Out Estuary Statewide, echo=F, results = 'hide'}}

estuaryAUs <- st_read('C:/HardDriveBackup/GIS/Assessment/va_2018_aus_estuarine.shp') %>%
   st_transform( st_crs(4326)) # transform to WQS84 for spatial intersection 

irData_missing_Estuary <- st_join(irData_missing, estuaryAUs, join = st_intersects) %>%
      filter(!is.na(OBJECTID))

#write.csv(irData_missing_Estuary, 'irData_missing_Estuary.csv', row.names = F) 
# you can change the file location like so:
#write.csv(irData_missing_Estuary, 'C:/Assessment/2020/dataProcessing/irData_missing_Estuary.csv', row.names = F) 

irData_missing_NoEstuary <- filter(irData_missing, !(FDT_STA_ID %in% irData_missing_Estuary$FDT_STA_ID))
rm(estuaryAUs) # remove from memory
```

Below is the table of sites that are estuarine.

```{r estuarySitesTable, echo=F}
DT::datatable(irData_missing_Estuary, escape=F, rownames = F, options = list(scrollX = TRUE))
#View(irData_missing_Estuary)
```

```{r inlineText2,echo=F}
cat(paste('Removing the above sites, the unique sites have gone from ', nrow(irData_missing),' to ',nrow(irData_missing_NoEstuary),'. The next step is to find the lake/reservoir stations and link them to the appropriate AU and WQS for analyses in the Lake Assessment App.', sep=''))
```


#### Last Cycle Reservoir Assessment Layer 

```{r inlineText3, echo=F}
cat(paste('Working with the ', nrow(irData_missing_NoEstuary),' sites that did not connect to a STATION_ID/FDT_STA_ID from last cycle, we will first do a spatial join to the reservoir/lakes AUs to see if any sites are lake sites before using the more computationally intensive stream snapping process.', sep=''))
```

The below chunk:
1) Reads in the 2018 reservoir AU layer
1.1) intersects assessmentLayer to get appropriate basin info
1.2) saves each layer in a spot to be accessed by app

This needs to be rerun each cycle with the most recent final AU information


```{r}
#reservoirAUs <- st_read('C:/HardDriveBackup/GIS/Assessment/va_2018_aus_reservoir.shp') %>%
#   st_transform( st_crs(4326)) %>% # transform to WQS84 for spatial intersection 
#  # intersect last cycle's lake AUs with assessmentLayer to get appropriate Basin argument for data organization
#   st_join(dplyr::select(assessmentLayer, ASSESS_REG, Basin), join = st_intersects) 

#for(i in 1:length(unique(reservoirAUs$ASSESS_REG))){
#  z <- filter(reservoirAUs, ASSESS_REG == as.character(unique(reservoirAUs$ASSESS_REG)[i]))
#  for(k in 1:length(unique(z$Basin))){
#    z1 <- filter(z, Basin == as.character(unique(z$Basin)[k]))
#    st_write(z1, paste0('data/processedGIS/va_2018_aus_reservoir_', 
#                          unique(z1$ASSESS_REG), "_", 
#                          unique(z1$Basin), '.shp'))
#  }
#}

```


```{r reservoirStatewide, echo=F, results = 'hide'}
# organize this one time for assessment and reuse layer
#reservoirAUs <- st_read('C:/HardDriveBackup/GIS/Assessment/va_2018_aus_reservoir.shp') %>%
#  st_transform( st_crs(4326)) %>% # transform to WQS84 for spatial intersection
#  # intersect last cycle's lake AUs with assessmentLayer to get appropriate Basin argument for data organization
#  st_join(dplyr::select(assessmentLayer, ASSESS_REG, Basin), join = st_intersects) 
#st_write(reservoirAUs, 'C:/HardDriveBackup/GIS/Assessment/va_2018_aus_reservoir_basins.shp')
reservoirAUs <- st_read('C:/HardDriveBackup/GIS/Assessment/va_2018_aus_reservoir_basins.shp') %>%
   filter(ASSESS_REG == as.character(assessmentRegion)) # and filter to region of interest

# holder for stations to drop from further analyses
stationsToRemove <- data.frame(FDT_STA_ID = NA, ID305B_Assigned = NA)

# Go through sites in each basin
for(i in 1:length(unique(reservoirAUs$Basin))){
  # find lake AUs, if Any
  irData_missing_Lakes <- st_join(filter(reservoirAUs, Basin == unique(reservoirAUs$Basin)[i]),
                                  irData_missing_NoEstuary, 
                                         join = st_intersects) %>%
    filter(!is.na(FDT_STA_ID)) %>% # only keep rows where join successful
    mutate(`Point Unique Identifier` = FDT_STA_ID, 
           `Buffer Distance` = 'Within Polygon') %>%
    dplyr::select(`Point Unique Identifier`, `Buffer Distance`, OBJECTID:Shape_Leng)
  
  # Organize all lake sites from previous assessment info
  lakeStations <- filter(irData_final, grepl('L_', ID305B_1) | grepl('L_', ID305B_2) | grepl('L_', ID305B_3) |
                           STATION_TYPE_1 == "L" | STATION_TYPE_2 == "L" | STATION_TYPE_3 == "L") %>%
    filter(Basin == unique(reservoirAUs$Basin)[i]) %>%
    rbind(filter(irData_join, FDT_STA_ID %in% irData_missing_Lakes$`Point Unique Identifier`))

  # save lake sites to special result for assessor to review, match riverine output
  # this is going to differ slightly from the riverine output in that tbl_output isn't necessary since all sites that don't fall into a lake (or estuary) AU will be
  # processed as riverine AND input_sites will contain stations that fell into lake AU and previously attributed lake sites
  lake_snapList_AU <- list('sf_output' = irData_missing_Lakes,
                           'inputSites' = lakeStations)
  
  stationFixes <- data.frame(FDT_STA_ID = as.character(irData_missing_Lakes$`Point Unique Identifier`), 
                             ID305B_Assigned = as.character(irData_missing_Lakes$ID305B))
  stationsToRemove <- bind_rows(stationsToRemove, stationFixes)
  
  saveRDS(lake_snapList_AU,paste0("data/preAnalyzedRegionalAUdata/", assessmentRegion,
                                  "/Lacustrine/", unique(reservoirAUs$Basin)[i], ".RDS"))

}


# and remove lake AU sites from the sites without AUs

irData_missing_NoAU <- mutate(irData_missing, ID305B_1 = NA) %>% # need column for snapping tools
  filter(!(FDT_STA_ID %in% stationsToRemove$FDT_STA_ID))

# update irData_join so fixed lake sites won't try to snap to riverine AUs
irData_join_fixed <- right_join(irData_join, stationsToRemove %>% drop_na(), by = 'FDT_STA_ID') %>%
  mutate(ID305B_1 = ID305B_Assigned,
         STATION_TYPE_1 = 'L') %>% 
  dplyr::select(-ID305B_Assigned) 
  

irData_join <- filter(irData_join, !FDT_STA_ID %in% irData_join_fixed$FDT_STA_ID) %>%
  rbind(irData_join_fixed)

rm(reservoirAUs) # remove from memory
```

#### Last Cycle Riverine Assessment Layer

```{r inlineText4, echo=F}
cat(paste('The last step for AU organization after we have removed any estuary sites and dealt with all sites that are in lake AUs is to automatically snap the', nrow(irData_missing_NoAU), 'remaining sites to riverine AUs.',sep=' '))
```

A custom, automated snapping tool is run below. The function buffers each site using the user defined sequence (bufferDistances). You do not need to change the bufferDistances variable, but commented out code demonstrates how you can if you want to increase/decrease the maximum buffer distance or breaks. 

The function first buffers the site the lowest input buffer distance, then it intersects that buffer with the input assessment unit layer. If no segments are retrieved in the given buffer distance, then the next highest buffer is tried and so on until the maximum buffer distance is tested. If one or more segments are captured in a given buffer, then the buffering process stops, returns the snapped stream segment information, and moves to the next site. If more that one site is captured in a single buffer, then those segments will be provided to the user to choose from in an interactive gadget to keep only one AU per site after all buffering has finished. If the maximum buffer distance does not yield any stream segments, then those sites will be returned to the user for special QA and manual table completion after the report is finished. It is advised that you investigate the site further and potentially correct site coordinates in CEDS to prevent future problems. 

**Note: All stations you want to use in the assessment applications require AU information. If no AU information is provided to the app with a StationID, then the station will not appear in the app and could potentially be lost until further QA.**

The next chunk will bring in the riverine AU layer from the previous cycle, complete the snapping tool, ask for user input in the form of a gadget in the viewer panel if sites snap to multiple segments in a single buffer, and then organize results. You can see the progress and results of the tool below the chunk.


The below chunk:
1) Reads in the 2018 reservoir AU layer
1.1) intersects assessmentLayer to get appropriate basin info
1.2) saves each layer in a spot to be accessed by app

This needs to be rerun each cycle with the most recent final AU information


```{r}
# use a custom snapping tool to snap and organize 
#riverineAUs <-  st_read('C:/HardDriveBackup/GIS/Assessment/va_2018_aus_riverine.shp') %>%
#  st_transform( st_crs(4326))  # transform to WQS84 for spatial intersection 

# intersect last cycle's lake AUs with assessmentLayer to get appropriate Basin argument for data organization
#riverineAUs <-  st_join(st_zm(riverineAUs), dplyr::select(assessmentLayer, ASSESS_REG, Basin), join = st_intersects) 


#for(i in 1:length(unique(riverineAUs$ASSESS_REG))){
#  z <- filter(riverineAUs, ASSESS_REG == as.character(unique(riverineAUs$ASSESS_REG)[i]))
#  for(k in 1:length(unique(z$Basin))){
#    z1 <- filter(z, Basin == as.character(unique(z$Basin)[k]))
#    st_write(z1, paste0('data/processedGIS/va_2018_aus_riverine_', 
#                          unique(z1$ASSESS_REG), "_", 
#                          unique(z1$Basin), '.shp'))
#  }
#}

```

```{r riverineAUsnap, echo=FALSE}
# use a custom snapping tool to snap and organize 
riverineAUs <-  st_read('C:/HardDriveBackup/GIS/Assessment/va_2018_aus_riverine.shp') %>%
  st_transform(crs = 102003)  # transform to Albers for spatial intersection

#Regional_Sites_AU <- snapAndOrganizeAU_noUI(filter(irData_missing_NoAU, FDT_STA_ID %in% #c('9-BFK003.33','9-LHC001.92'), riverineAUs,  bufferDistances = seq(10,50,by=10))

snapAndOrganizeAU_newOutput(irData_join, riverineAUs, 
                            bufferDistances = seq(10,50,by=10),
                            outDir = 'data/preAnalyzedRegionalAUdata/BRRO/Riverine/')
#Regional_Sites_AU <- 
#saveRDS(Regional_Sites_AU, 'data/preAnalyzedRegionalAUdata/BRRO/Riverine/James River Basin_snapList.RDS')

rm(riverineAUs) #clean up workspace
```







### WQS Work

Now that all stations are at least organized for AU attribution, we need to do the same for WQS attribution. Like before, we will use whatever assessor QAed information we can get our hands on as a starting point to reduce the amount of spatial snapping required.

Starting with irData_join bc that has the sites that have WQS info already

```{r}

irData_missingWQS <- filter(irData_join, is.na(CLASS)) # these had no previous records

irData_finalWQS <- filter(irData_join, !is.na(CLASS))

# make sure we didn't lose anyone
nrow(irData_finalWQS) + nrow(irData_missingWQS) == nrow(irData_join)

```


