---
title: "How to run automated assessment"
author: "Emma Jones"
date: "October 14, 2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

options(digits = 12) #critical to seeing all the data

library(tidyverse)
library(sf)
library(readxl)
library(pins)
library(config)
library(EnvStats)
#library(FSA)
library(lubridate)
#library(magrittr)

# Bring in Assessment functions from app
source('global.R')

# get configuration settings
conn <- config::get("connectionSettings")

# use API key to register board
board_register_rsconnect(key = conn$CONNECT_API_KEY,  #Sys.getenv("CONNECT_API_KEY"),
                          server = conn$CONNECT_SERVER)#Sys.getenv("CONNECT_SERVER"))

```

This document walks users through running the automated assessement for any given set of stations and waterbody type (riverine or lacustrine for now). Prior to this point, the user needs to have completed the necessary prerequisites of attributing stations with AU and WQS information. This dataset is a companion to the rivers and streams/lake assessment applications and is **NOT** final. The results cited in this table are identical to the app calculations and are meant for assessor review, QA, editing prior to sumbitting to WQA CEDS bulk data upload.

# Prerequisites
The user will have to have all conventionals data, water column metals, and sediment metals organized by Roger for the window. **Additionally, the CEDS EDAS data needs to be exported and available for use, including regional Bioassessment calls**. Last cycle's finalized regional assessment layer (shapefile with AUs) are the spatial component needed. Lastly, the assessor must have the stations bulk data upload table filled out to their requirements in order to run the assessment scripts. 

## Pin Data/Retrieve Data

Scripts to organize and pin data are in ../2.organizeMetadata/organizeStationMetadata_DRAFTforApplication.Rmd 

## Bring in Pinned data

Much faster method and makes sure data used by assessors is same for testing.

```{r pull pins}
conventionals <- pin_get("conventionals2022IRdraft", board = "rsconnect")
conventionals_distinct <- pin_get("conventionals-distinct-draft", board = "rsconnect")
stations2020IR <- pin_get("stations2020IR-sf-draft", board = "rsconnect")
VSCIresults <- pin_get("VSCIresults", board = "rsconnect")
VCPMI63results <- pin_get("VCPMI63results", board = "rsconnect")
VCPMI65results <- pin_get("VCPMI65results", board = "rsconnect")
WCmetals <- pin_get("WCmetals-2020IRfinal",  board = "rsconnect")
Smetals <- pin_get("Smetals-2020IRfinal",  board = "rsconnect")
```

## Bring in Station Table Bulk Upload Template

```{r bulk upload template}
stationsTemplate <- read_excel('WQA_CEDS_templates/WQA_Bulk_Station_Upload (3).xlsx', 
                               sheet = 'Stations', col_types = "text")[0,] # just want structure and not the draft data, force everything to character for now

```



# Workflow

The following steps complete the automated assessment.

## Bring in user station table data

This information communicates to the scripts which stations should be assessed and where they should be organized (AUs).

```{r stationTable}
stationTable <- read_csv('processedStationData/stationsTable2022begin.csv')
```

## Attach WQS information

Is there a better way to do this?

Pull pinned WQS info saved on server and join to station table. Identify if any stations are missing WQS. 

```{r pull WQS info}
WQSlookup <- pin_get("WQSlookup-withStandards",  board = "rsconnect")

stationTable <- left_join(stationTable, WQSlookup, by = c('STATION_ID'='StationID')) %>%
  mutate(CLASS_BASIN = paste(CLASS,substr(BASIN, 1,1), sep="_")) %>%
  mutate(CLASS_BASIN = ifelse(CLASS_BASIN == 'II_7', "II_7", as.character(CLASS))) %>%
  # Fix for Class II Tidal Waters in Chesapeake (bc complicated DO/temp/etc standard)
  left_join(WQSvalues, by = 'CLASS_BASIN') %>%
  dplyr::select(-c(CLASS.y,CLASS_BASIN)) %>%
  rename('CLASS' = 'CLASS.x') 
  # last cycle had code to fix Class II Tidal Waters in Chesapeake (bc complicated DO/temp/etc standard) but not sure if necessary
  

missingWQS <- filter(stationTable, is.na(CLASS))
```



# steps

pin conventionals, metals, benthics, wqs information, last cycle AU info (spatial)

user upload AU and station info (carry over only what is given to scripts, NA if nothing given)

script joins WQS info to conventionals

go through conventionals one station at a time, join stationTable, analyze by functions, report in WQA CEDS template


```{r}
stationTableResults <- stationsTemplate

# time it:
startTime <- Sys.time()

# loop over all sites, not super efficient but get the job done for now
for(i in 1:200){#nrow(stationTable)){
#i = 1
  print(paste('Assessing station', i, 'of', nrow(stationTable), sep=' '))

  # pull one station data
  stationData <- filter(conventionals, FDT_STA_ID %in% stationTable$STATION_ID[i]) %>%
    left_join(stationTable, by = c('FDT_STA_ID' = 'STATION_ID'))
  
  
  results <- cbind(
    StationTableStartingData(stationData),
    tempExceedances(stationData),
    DOExceedances_Min(stationData), 
    pHExceedances(stationData),
    bacteriaExceedances_NEW(citmonOutOfBacteria(stationData, E.COLI, ECOLI_RMK),'E.COLI', 10, 410, 126),
    bacteriaExceedances_NEW(citmonOutOfBacteria(stationData, ENTEROCOCCI, RMK_31649),'ENTEROCOCCI',10, 35, 104)
    ) %>%
    dplyr::select(-ends_with('exceedanceRate')) # to match Bulk Upload template but helpful to keep visible til now for testing
  
  stationTableResults <- rbind(stationTableResults,results)
}

timeDiff = Sys.time()- startTime
  
```






