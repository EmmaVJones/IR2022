---
title: "How to run automated assessment"
author: "Emma Jones"
date: "October 14, 2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

options(digits = 12) #critical to seeing all the data

library(tidyverse)
library(sf)
library(readxl)
library(pins)
library(config)
library(EnvStats)
#library(FSA)
library(lubridate)
#library(magrittr)

# Bring in Assessment functions from app
source('automatedAssessmentFunctions.R')
source('updatedBacteriaCriteria.R')

# get configuration settings
conn <- config::get("connectionSettings")

# use API key to register board
board_register_rsconnect(key = conn$CONNECT_API_KEY,  #Sys.getenv("CONNECT_API_KEY"),
                          server = conn$CONNECT_SERVER)#Sys.getenv("CONNECT_SERVER"))

```

This document walks users through running the automated assessement for any given set of stations and waterbody type (riverine or lacustrine for now). Prior to this point, the user needs to have completed the necessary prerequisites of attributing stations with AU and WQS information. This dataset is a companion to the rivers and streams/lake assessment applications and is **NOT** final. The results cited in this table are identical to the app calculations and are meant for assessor review, QA, editing prior to sumbitting to WQA CEDS bulk data upload.

# Prerequisites
The user will have to have all conventionals data, water column metals, and sediment metals organized by Roger for the window. **Additionally, the CEDS EDAS data needs to be exported and available for use, including regional Bioassessment calls**. Last cycle's finalized regional assessment layer (shapefile with AUs) are the spatial component needed. Lastly, the assessor must have the stations bulk data upload table filled out to their requirements in order to run the assessment scripts. 

## Pin Data/Retrieve Data

Scripts to organize and pin data are in ../2.organizeMetadata/organizeStationMetadata_DRAFTforApplication.Rmd 

## Bring in Pinned data

Much faster method and makes sure data used by assessors is same for testing.

```{r pull pins}
conventionals <- pin_get("conventionals2022IRdraft", board = "rsconnect")
conventionals_distinct <- pin_get("conventionals-distinct-draft", board = "rsconnect")
stations2020IR <- pin_get("stations2020IR-sf-draft", board = "rsconnect")
WQMstationFull <- pin_get("WQM-Station-Full", board = "rsconnect")
VSCIresults <- pin_get("VSCIresults", board = "rsconnect") %>%
  filter( between(`Collection Date`, assessmentPeriod[1], assessmentPeriod[2]) )
VCPMI63results <- pin_get("VCPMI63results", board = "rsconnect") %>%
  filter( between(`Collection Date`, assessmentPeriod[1], assessmentPeriod[2]) )
VCPMI65results <- pin_get("VCPMI65results", board = "rsconnect") %>%
  filter( between(`Collection Date`, assessmentPeriod[1], assessmentPeriod[2]) ) 
WCmetals <- pin_get("WCmetals-2020IRfinal",  board = "rsconnect")
Smetals <- pin_get("Smetals-2020IRfinal",  board = "rsconnect")
```

## Bring in Station Table Bulk Upload Template

```{r bulk upload template}
stationsTemplate <- read_excel('WQA_CEDS_templates/WQA_Bulk_Station_Upload (3).xlsx', 
                               sheet = 'Stations')[0,]#, col_types = "text")[0,] %>%  # just want structure and not the draft data, force everything to character for now
#  mutate(LATITUDE= as.numeric(LATITUDE), LONGITUDE = as.numeric(LONGITUDE))

```



# Workflow

The following steps complete the automated assessment.

## Bring in user station table data

This information communicates to the scripts which stations should be assessed and where they should be organized (AUs). It also has data from the last cycle to populate the historical station information table in the application

```{r stationTable}
stationTable <- read_csv('processedStationData/stationsTable2022begin.csv') 
```

## Attach WQS information

Is there a better way to do this?

Pull pinned WQS info saved on server and join to station table. Identify if any stations are missing WQS. 

```{r pull WQS info}
WQSlookup <- pin_get("WQSlookup-withStandards",  board = "rsconnect")

stationTable <- left_join(stationTable, WQSlookup, by = c('STATION_ID'='StationID')) %>%
  mutate(CLASS_BASIN = paste(CLASS,substr(BASIN, 1,1), sep="_")) %>%
  mutate(CLASS_BASIN = ifelse(CLASS_BASIN == 'II_7', "II_7", as.character(CLASS))) %>%
  # Fix for Class II Tidal Waters in Chesapeake (bc complicated DO/temp/etc standard)
  left_join(WQSvalues, by = 'CLASS_BASIN') %>%
  dplyr::select(-c(CLASS.y,CLASS_BASIN)) %>%
  rename('CLASS' = 'CLASS.x') %>%
  left_join(dplyr::select(WQMstationFull, WQM_STA_ID, EPA_ECO_US_L3CODE, EPA_ECO_US_L3NAME) %>%
              distinct(WQM_STA_ID, .keep_all = TRUE), by = c('STATION_ID' = 'WQM_STA_ID'))
  # last cycle had code to fix Class II Tidal Waters in Chesapeake (bc complicated DO/temp/etc standard) but not sure if necessary
  


missingWQS <- filter(stationTable, is.na(CLASS))
```


We only want to run riverine methods on riverine stations right now, so drop all stations that have lake AU designations

```{r}
lakeStations <- filter(stationTable, str_detect(ID305B_1, 'L_') | str_detect(ID305B_2, 'L_') | str_detect(ID305B_3, 'L_') | str_detect(ID305B_4, 'L_') |
                          str_detect(ID305B_5, 'L_') | str_detect(ID305B_6, 'L_') | str_detect(ID305B_7, 'L_') | str_detect(ID305B_8, 'L_') | 
                          str_detect(ID305B_9, 'L_') | str_detect(ID305B_10, 'L_') ) 
 bothTypes <- filter(lakeStations, str_detect(ID305B_1, 'R_') | str_detect(ID305B_2, 'R_') | str_detect(ID305B_3, 'R_') | str_detect(ID305B_4, 'R_') |
                          str_detect(ID305B_5, 'R_') | str_detect(ID305B_6, 'R_') | str_detect(ID305B_7, 'R_') | str_detect(ID305B_8, 'R_') | 
                          str_detect(ID305B_9, 'R_') | str_detect(ID305B_10, 'R_') ) 
stationTable <- filter(stationTable, ! STATION_ID %in% lakeStations$STATION_ID)

```





For testing reduce stationTable to just BRRO for now

```{r}
#stationTable <- filter(stationTable, REGION == "BRRO")
```


# steps

pin conventionals, metals, benthics, wqs information, last cycle AU info (spatial)

user upload AU and station info (carry over only what is given to scripts, NA if nothing given)

script joins WQS info to conventionals

go through conventionals one station at a time, join stationTable, analyze by functions, report in WQA CEDS template


```{r}
stationTableResults <- stationsTemplate

# time it:
startTime <- Sys.time()

# loop over all sites, not super efficient but get the job done for now
for(i in 1:nrow(stationTable)){
#i = 1
  print(paste('Assessing station', i, 'of', nrow(stationTable), sep=' '))

  # pull one station data
  stationData <- filter(conventionals, FDT_STA_ID %in% stationTable$STATION_ID[i]) %>%
    left_join(stationTable, by = c('FDT_STA_ID' = 'STATION_ID')) %>%
  pHSpecialStandardsCorrection() # correct pH to special standards where necessary
  
  # If data exists for station, run it, otherwise just output what last cycle said and comment
  if(nrow(stationData) > 0){
    results <- cbind(
      StationTableStartingData(stationData),
      tempExceedances(stationData) %>% quickStats('TEMP'),
      DOExceedances_Min(stationData) %>% quickStats('DO'), 
      pHExceedances(stationData) %>% quickStats('PH'),
      bacteriaAssessmentDecision(stationData, 'ECOLI', 'RMK_ECOLI', 10, 410, 126) %>%
        dplyr::select(ECOLI_EXC:ECOLI_STATECOLI_VERBOSE), # add verbose comment to COMMENT field
        #dplyr::select(ECOLI_EXC:ECOLI_STAT), # don't add verbose comment to COMMENT field
      bacteriaAssessmentDecision(stationData, 'ENTEROCOCCI', 'RMK_ENTEROCOCCI', 10, 130, 35) %>%
        dplyr::select(ENTER_EXC:ENTER_STATENTER_VERBOSE), # add verbose comment to COMMENT field
        #dplyr::select(ENTER_EXC:ENTER_STAT), # don't add verbose comment to COMMENT field
    
      # Ammonia needs to go here
      # AMMONIA_EXC, AMMONIA_STAT
    
      # Exact copy of last cycle's scripts for WAT_MET_EXC, WAT_MET_STAT, 
      metalsExceedances(filter(WCmetals, FDT_STA_ID %in% stationData$FDT_STA_ID) %>% 
                                       dplyr::select(`ANTIMONY HUMAN HEALTH PWS`:`ZINC ALL OTHER SURFACE WATERS`), 'WAT_MET'),
      # Need more clarification on water column metals, what comes in, what desired in columns
      # WAT_TOX_EXC, WAT_TOX_STAT
    
      # Exact copy of last cycle's scripts for SED_MET_EXC, SED_MET_STAT
      metalsExceedances(filter(Smetals, FDT_STA_ID %in% stationData$FDT_STA_ID) %>% 
                                      dplyr::select(ARSENIC:ZINC), 'SED_MET'), 
      # Need more clarification on sediment metals, what comes in, what desired in columns
      # SED_TOX_EXC, SED_TOX_STAT
    
      # Need more clarification on Fish, what comes in, what desired in columns
      # FISH_MET_EXC, FISH_MET_STAT, FISH_TOX_EXC, FISH_TOX_STAT
    
      # Benthics 
      # BENTHIC_STAT, BENTHIC_WOE_CAT, BIBI_SCORE
      benthicAssessment(stationData, VSCIresults),
    
      # Nutrient: TP (lakes have real standards but riverine uses 0.2 mg/L as an observed effect for Aquatic life use)
      # NUT_TP_EXC, NUT_TP_SAMP, NUT_TP_STAT
      countNutrients(stationData, PHOSPHORUS, RMK_PHOSPHORUS, 0.2) %>% quickStats('NUT_TP') %>% 
        mutate(NUT_TP_STAT = ifelse(NUT_TP_STAT != "S", "Review", NA)), # flag OE but don't show a real assessment decision
    
      # Nutrients: Chl a (lakes)
      #NUT_CHLA_EXC, NUT_CHLA_SAMP, NUT_CHLA_STAT
      countNutrients(stationData, CHLOROPHYLL, RMK_CHLOROPHYLL, NA) %>% quickStats('NUT_CHLA') %>%
        mutate(NUT_CHLA_STAT = NA)) %>% # don't show a real assessment decision
    
      # COMMENTS
      mutate(COMMENTS = paste0('ECOLI Comment: ', ECOLI_STATECOLI_VERBOSE,
                               ' | ENTEROCOCCI Comment: ', ENTER_STATENTER_VERBOSE)) %>%
      dplyr::select(-ends_with(c('exceedanceRate', 'VERBOSE'))) # to match Bulk Upload template but helpful to keep visible til now for testing
  } else {# pull what you can from last cycle and flag as carry over
    results <- filter(stationTable, STATION_ID == stationTable$STATION_ID[i]) %>%
      dplyr::select(STATION_ID:VAHU6, COMMENTS) %>%
      mutate(COMMENTS= paste0('This station has no data in current window but was carried over due to IM in one of the 2020IR status fields or the 2020 stations table reports the station was carried over from a previous cycle. The 2020 comment reads: ', COMMENTS))
  }
  
  stationTableResults <- bind_rows(stationTableResults,results)
    
}


stationTableResults <- bind_rows(stationsTemplate, stationTableResults) %>%
  # for now bc bacteria needs help still
  dplyr::select(STATION_ID:COMMENTS)

timeDiff = Sys.time()- startTime
  
```


Now this is the starting point for the app.

```{r}
write_csv(stationTableResults, 'processedStationData/stationTableResults.csv',
          na = "") # dont write out a character NA in csv

stationTableResults1 <- filter(stationTableResults, STATION_ID %in% filter(stationTable, REGION == "BRRO")$STATION_ID)
write_csv(stationTableResults1, 'processedStationData/stationTableResultsBRRO.csv',
          na = "") # dont write out a character NA in csv

```


